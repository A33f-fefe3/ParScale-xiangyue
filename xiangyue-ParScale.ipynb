{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ff29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024, presence_penalty=1.5):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_output = llm_qwen(\"ä½ å¥½\")\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import concurrent.futures\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class AggregationLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_perspectives=3):\n",
    "        super().__init__()\n",
    "        self.aggregate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        concat_responses = torch.cat(encoded_responses, dim=0).unsqueeze(0)\n",
    "        weights = self.softmax(self.aggregate_layer(concat_responses))\n",
    "        weighted_sum = torch.sum(\n",
    "            torch.stack(encoded_responses) * weights.squeeze(0).unsqueeze(1), \n",
    "            dim=0\n",
    "        )\n",
    "        return weighted_sum, weights\n",
    "\n",
    "def load_encoder_model():\n",
    "    model_name = \"/app/sda1/xiangyue/model/chinese-lert-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "def process_perspective(key, question, model, tokenizer, llm_qwen):\n",
    "    response = llm_qwen(question)\n",
    "    encoded_response = encode_text(response, model, tokenizer)\n",
    "    return key, response, encoded_response\n",
    "\n",
    "def generate_perspectives(metaprompt, p):\n",
    "    return f\"\"\"\n",
    "    å¯¹è¿™ä¸ªé—®é¢˜{metaprompt}åŠ ä¸Š{p}ä¸åŒè§†è§’çš„å‰ç¼€æ ‡ç­¾ï¼Œæ³¨æ„ä¸è¦æ”¹å˜åŸå¥å­ï¼Œå½¢æˆ{p}ä¸ªé—®é¢˜\n",
    "    åªè¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\n",
    "    {{\n",
    "        \"one\": \"\",\n",
    "        \"two\": \"\",\n",
    "        \"three\": \"\",\n",
    "        ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p, topk, llm_qwen):\n",
    "    # ç”Ÿæˆå¹¶è§£æè§†è§’é—®é¢˜\n",
    "    perspectives = json.loads(llm_qwen(generate_perspectives(metaprompt, p)))\n",
    "    print(\"åˆå§‹è§†è§’é—®é¢˜ç”Ÿæˆï¼š\")\n",
    "    print(json.dumps(perspectives, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    aggregation_model = AggregationLayer(hidden_size=1024, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "\n",
    "    # å¹¶è¡Œå¤„ç†è§†è§’é—®é¢˜\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_perspective, key, question, encoder_model, tokenizer, llm_qwen)\n",
    "            for key, question in perspectives.items()\n",
    "        ]\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    # æå–ç»“æœ\n",
    "    responses = [(key, response) for key, response, _ in results]\n",
    "    encoded_responses = [encoded for _, _, encoded in results]\n",
    "    response_texts = [response for _, response, _ in results]\n",
    "    \n",
    "    # å¦‚æœæœ‰æœ‰æ•ˆå›ç­”ï¼Œæ‰§è¡Œèšåˆ\n",
    "    if encoded_responses:\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # æ‰“å°æƒé‡ä¿¡æ¯\n",
    "        print(\"\\nèšåˆæƒé‡:\")\n",
    "        for i, (key, _) in enumerate(responses):\n",
    "            print(f\"{key}: {weights[0, i].item():.4f}\")\n",
    "        \n",
    "        # è·å–å¹¶æ‰“å°topkè§†è§’\n",
    "        top_weights, top_indices = torch.topk(weights[0], topk)\n",
    "        print(f\"\\nTop {topk} è§†è§’:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            key = responses[idx][0]\n",
    "            print(f\"{key}: {top_weights[i].item():.4f}\")\n",
    "        \n",
    "        # ç”Ÿæˆæœ€ç»ˆç»“æœ\n",
    "        final_title = \"\\n\".join(\n",
    "            re.sub(r'ã€[^ã€‘]+ã€‘', '', response_texts[idx]).strip()\n",
    "            for idx in top_indices\n",
    "        )\n",
    "        print(\"\\næœ€ç»ˆèšåˆç»“æœ:\")\n",
    "        print(final_title)\n",
    "        \n",
    "        return {\n",
    "            \"final_title\": final_title,\n",
    "            \"top_perspectives\": [(responses[idx][0], top_weights[i].item()) for i, idx in enumerate(top_indices)],\n",
    "            \"all_weights\": [(responses[i][0], weights[0, i].item()) for i in range(len(responses))]\n",
    "        }\n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æœ‰æ•ˆçš„å›ç­”å¯ä¾›èšåˆ\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metaprompt = \"ä½ å¥½\"\n",
    "    result = multi_perspective_analysis(metaprompt, p=5, topk=1, llm_qwen=llm_qwen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
