{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ff29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮你的吗？😊\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=2048, presence_penalty=1.5):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_output = llm_qwen(\"你好\")\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6777fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Optional, Union\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# 下载NLTK所需资源（首次运行时取消注释）\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "class ComplexityWeightedAggregationLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, complexity_weight_ratio=0.7):\n",
    "        \"\"\"\n",
    "        基于复杂度加权的聚合层：\n",
    "        - 保留全局聚合路径\n",
    "        - 用文本复杂度替代注意力权重\n",
    "        - complexity_weight_ratio控制复杂度权重影响程度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.complexity_weight_ratio = complexity_weight_ratio\n",
    "        \n",
    "        # 第一级：两两视角聚合\n",
    "        self.pairwise_aggregators = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size * 2, hidden_size),\n",
    "                torch.nn.LayerNorm(hidden_size),\n",
    "                torch.nn.GELU()\n",
    "            )\n",
    "            for _ in range(num_perspectives * (num_perspectives - 1) // 2)\n",
    "        ])\n",
    "        \n",
    "        # 第二级：全局聚合\n",
    "        num_pairwise = num_perspectives * (num_perspectives - 1) // 2\n",
    "        self.global_aggregator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_pairwise, hidden_size * 2),\n",
    "            torch.nn.LayerNorm(hidden_size * 2),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.15),\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # 文本复杂度计算模型（外部传入）\n",
    "        self.complexity_model = None\n",
    "        self.complexity_tokenizer = None\n",
    "    \n",
    "    def set_complexity_tools(self, model: AutoModel, tokenizer: AutoTokenizer):\n",
    "        \"\"\"设置复杂度计算所需的模型和分词器\"\"\"\n",
    "        self.complexity_model = model\n",
    "        self.complexity_tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self, encoded_responses: List[torch.Tensor], response_texts: List[str]):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        \n",
    "        参数：\n",
    "            encoded_responses: 视角特征列表 [num_perspectives, hidden_size]\n",
    "            response_texts: 视角文本列表 [num_perspectives]\n",
    "        \"\"\"\n",
    "        stacked = torch.stack(encoded_responses)  # [num_perspectives, hidden_size]\n",
    "        \n",
    "        # 第一阶段：两两视角聚合\n",
    "        pairwise_outputs = []\n",
    "        idx = 0\n",
    "        for i in range(self.num_perspectives):\n",
    "            for j in range(i + 1, self.num_perspectives):\n",
    "                pair = torch.cat([stacked[i], stacked[j]], dim=-1).unsqueeze(0)\n",
    "                pairwise_outputs.append(self.pairwise_aggregators[idx](pair))\n",
    "                idx += 1\n",
    "        \n",
    "        # 第二阶段：全局聚合\n",
    "        pairwise_concat = torch.cat(pairwise_outputs, dim=-1)  # [1, hidden_size*num_pairs]\n",
    "        global_repr = self.global_aggregator(pairwise_concat)  # [1, hidden_size]\n",
    "        \n",
    "        # 第三阶段：基于复杂度的权重计算\n",
    "        complexities = [\n",
    "            calculate_complexity(text, self.complexity_model, self.complexity_tokenizer)\n",
    "            for text in response_texts\n",
    "        ]\n",
    "        \n",
    "        # 归一化为概率分布\n",
    "        complexities = torch.tensor(complexities, dtype=torch.float32)\n",
    "        weights = torch.softmax(complexities, dim=-1)  # [num_perspectives]\n",
    "        \n",
    "        # 原始视角的加权和\n",
    "        weighted_aggregation = torch.sum(\n",
    "            stacked * weights.t().unsqueeze(-1),\n",
    "            dim=0\n",
    "        )  # [hidden_size]\n",
    "        \n",
    "        # 最终输出融合\n",
    "        final_output = (self.complexity_weight_ratio * weighted_aggregation.unsqueeze(0) + \n",
    "                       (1 - self.complexity_weight_ratio) * global_repr)  # [1, hidden_size]\n",
    "        \n",
    "        return final_output, weights\n",
    "\n",
    "def load_encoder_model() -> Tuple[AutoModel, AutoTokenizer]:\n",
    "    \"\"\"加载预训练的BERT编码器和分词器\"\"\"\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    return AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_text(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> torch.Tensor:\n",
    "    \"\"\"将文本编码为BERT的CLS向量\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze(0)  # [hidden_size]\n",
    "\n",
    "def calculate_complexity(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> float:\n",
    "    \"\"\"\n",
    "    计算文本复杂度（多维度指标融合）：\n",
    "    1. 文本长度复杂度\n",
    "    2. 词汇多样性（TF-IDF熵）\n",
    "    3. 语义嵌入方差\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    # 1. 文本长度复杂度（对数归一化）\n",
    "    word_count = len(word_tokenize(text))\n",
    "    length_score = np.log(word_count + 1) / np.log(100 + 1)  # 归一化到[0,1]\n",
    "    \n",
    "    # 2. 词汇多样性（TF-IDF熵）\n",
    "    words = [word for word in word_tokenize(text) if word.isalpha() and len(word) > 1]\n",
    "    if len(words) < 2:\n",
    "        diversity_score = 0.0\n",
    "    else:\n",
    "        word_freq = Counter(words)\n",
    "        probs = np.array(list(word_freq.values()), dtype=float) / len(words)\n",
    "        diversity_score = -np.sum(probs * np.log(probs + 1e-10)) / np.log(len(word_freq))\n",
    "        diversity_score = max(0.0, min(1.0, diversity_score))  # 裁剪到[0,1]\n",
    "    \n",
    "    # 3. 语义嵌入方差（BERT各token向量的方差）\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.squeeze(0)\n",
    "        valid_mask = inputs.input_ids.squeeze(0) != 0  # 忽略padding\n",
    "        valid_embeddings = embeddings[valid_mask]\n",
    "        if len(valid_embeddings) < 2:\n",
    "            semantic_variance = 0.0\n",
    "        else:\n",
    "            variance = torch.var(valid_embeddings, dim=0).mean().item()\n",
    "            # 动态范围归一化（假设方差通常在0.01-1.0之间）\n",
    "            semantic_variance = (variance - 0.01) / (1.0 - 0.01) if variance > 0.01 else 0.0\n",
    "            semantic_variance = max(0.0, min(1.0, semantic_variance))  # 裁剪到[0,1]\n",
    "    \n",
    "    # 综合复杂度分数（各指标加权求和）\n",
    "    complexity_score = 0.3 * length_score + 0.3 * diversity_score + 0.4 * semantic_variance\n",
    "    return max(complexity_score, 1e-6)  # 防止全零\n",
    "\n",
    "def process_perspective(\n",
    "    index: int, \n",
    "    question: str, \n",
    "    encoder_model: AutoModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    llm: callable, \n",
    "    temperature: float\n",
    ") -> Tuple[int, str, torch.Tensor]:\n",
    "    \"\"\"处理单个视角：生成回答 -> 编码向量\"\"\"\n",
    "    response = llm(f\"{question}\", temperature=temperature)\n",
    "    encoded = encode_text(response, encoder_model, tokenizer)\n",
    "    return index, response, encoded\n",
    "\n",
    "def multi_perspective_analysis(\n",
    "    metaprompt: str, \n",
    "    p: int = 3, \n",
    "    topk: int = 1, \n",
    "    llm: callable = None, \n",
    "    temperature_settings: List[float] = None,\n",
    "    complexity_weight_ratio: float = 0.4,\n",
    "    hidden_size: int = 768,\n",
    "    length_weight: float = 0.3,\n",
    "    diversity_weight: float = 0.3,\n",
    "    variance_weight: float = 0.4,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    多视角分析主函数：\n",
    "    1. 并行生成p个视角回答\n",
    "    2. 计算各视角复杂度权重\n",
    "    3. 聚合特征并返回topk视角\n",
    "    \n",
    "    参数:\n",
    "        metaprompt: 元提示文本\n",
    "        p: 视角数量\n",
    "        topk: 返回的顶部视角数量\n",
    "        llm: 大语言模型调用函数\n",
    "        temperature_settings: 各视角的温度设置\n",
    "        complexity_weight_ratio: 复杂度权重占比\n",
    "        hidden_size: 隐藏层大小\n",
    "        length_weight: 文本长度复杂度权重\n",
    "        diversity_weight: 词汇多样性权重\n",
    "        variance_weight: 语义嵌入方差权重\n",
    "    \"\"\"\n",
    "    global calculate_complexity\n",
    "    \n",
    "    if llm is None:\n",
    "        llm = llm_qwen  # 默认使用模拟LLM\n",
    "    \n",
    "    # 处理temperature设置\n",
    "    if temperature_settings is None:\n",
    "        temperatures = [0.1] * p\n",
    "    elif isinstance(temperature_settings, (int, float)):\n",
    "        temperatures = [temperature_settings] * p\n",
    "    elif len(temperature_settings) == p:\n",
    "        temperatures = temperature_settings\n",
    "    else:\n",
    "        raise ValueError(\"temperature_settings需为单个值或长度为p的列表\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # 创建闭包以修改复杂度计算的权重\n",
    "    original_calculate_complexity = calculate_complexity\n",
    "    def calculate_complexity_with_weights(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> float:\n",
    "        \"\"\"使用自定义权重的文本复杂度计算函数\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        # 1. 文本长度复杂度\n",
    "        word_count = len(word_tokenize(text))\n",
    "        length_score = np.log(word_count + 1) / np.log(100 + 1)\n",
    "        \n",
    "        # 2. 词汇多样性\n",
    "        words = [word for word in word_tokenize(text) if word.isalpha() and len(word) > 1]\n",
    "        if len(words) < 2:\n",
    "            diversity_score = 0.0\n",
    "        else:\n",
    "            word_freq = Counter(words)\n",
    "            probs = np.array(list(word_freq.values()), dtype=float) / len(words)\n",
    "            diversity_score = -np.sum(probs * np.log(probs + 1e-10)) / np.log(len(word_freq))\n",
    "            diversity_score = max(0.0, min(1.0, diversity_score))\n",
    "        \n",
    "        # 3. 语义嵌入方差\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**inputs).last_hidden_state.squeeze(0)\n",
    "            valid_mask = inputs.input_ids.squeeze(0) != 0\n",
    "            valid_embeddings = embeddings[valid_mask]\n",
    "            if len(valid_embeddings) < 2:\n",
    "                semantic_variance = 0.0\n",
    "            else:\n",
    "                variance = torch.var(valid_embeddings, dim=0).mean().item()\n",
    "                semantic_variance = (variance - 0.01) / (1.0 - 0.01) if variance > 0.01 else 0.0\n",
    "                semantic_variance = max(0.0, min(1.0, semantic_variance))\n",
    "        \n",
    "        # 使用传入的权重\n",
    "        complexity_score = (length_weight * length_score + \n",
    "                           diversity_weight * diversity_score + \n",
    "                           variance_weight * semantic_variance)\n",
    "        return max(complexity_score, 1e-6)\n",
    "    \n",
    "    # 临时替换复杂度计算函数\n",
    "    calculate_complexity = calculate_complexity_with_weights\n",
    "    \n",
    "    aggregation_model = ComplexityWeightedAggregationLayer(\n",
    "        num_perspectives=p, \n",
    "        complexity_weight_ratio=complexity_weight_ratio,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "    aggregation_model.set_complexity_tools(encoder_model, tokenizer)\n",
    "    aggregation_model.eval()  # 设置为评估模式\n",
    "    \n",
    "    # 并行处理多视角\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, \n",
    "                i, \n",
    "                metaprompt, \n",
    "                encoder_model, \n",
    "                tokenizer, \n",
    "                llm, \n",
    "                temperatures[i]\n",
    "            ) for i in range(p)\n",
    "        ]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    # 恢复原始的复杂度计算函数\n",
    "    calculate_complexity = original_calculate_complexity\n",
    "    \n",
    "    # 整理结果\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # 打印各视角信息（含复杂度）\n",
    "    print(\"\\n=== 各视角分析结果 ===\")\n",
    "    complexities = [\n",
    "        calculate_complexity_with_weights(text, encoder_model, tokenizer)\n",
    "        for text in response_texts\n",
    "    ]\n",
    "    for idx, (text, temp, comp) in enumerate(zip(response_texts, temperatures, complexities), start=1):\n",
    "        # print(f\"视角{idx} (temp={temp:.2f}, 复杂度={comp:.4f}):\")\n",
    "        print(f\"视角{idx} temp={temp:.2f}\")\n",
    "        print(f\"  {text}\\n\")\n",
    "    \n",
    "    if not encoded_responses:\n",
    "        return {\"error\": \"未获取到有效视角\"}\n",
    "    \n",
    "    # 执行复杂度加权聚合\n",
    "    with torch.no_grad():\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses, response_texts)\n",
    "    \n",
    "    # 整理topk结果\n",
    "    sorted_items = sorted(\n",
    "        [(i, f\"视角{i+1}\", weights[i].item(), complexities[i]) for i in range(p)],\n",
    "        key=lambda x: x[2], reverse=True\n",
    "    )\n",
    "    top_indices = [item[0] for item in sorted_items[:topk]]\n",
    "    top_perspectives = [\n",
    "        {\n",
    "            \"视角\": f\"视角{idx+1}\",\n",
    "            \"权重\": weights[idx].item(),\n",
    "            \"复杂度\": complexities[idx],\n",
    "            \"回答\": response_texts[idx]\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"总视角数\": p,\n",
    "        \"topk结果\": top_perspectives,\n",
    "        \"所有视角权重\": [(item[1], item[2]) for item in sorted_items],\n",
    "        \"复杂度指标\": [(item[1], item[3]) for item in sorted_items],\n",
    "        \"参数配置\": {\n",
    "            \"complexity_weight_ratio\": complexity_weight_ratio,\n",
    "            \"length_weight\": length_weight,\n",
    "            \"diversity_weight\": diversity_weight,\n",
    "            \"variance_weight\": variance_weight,\n",
    "            \"temperature_settings\": temperatures\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    metaprompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # 手动设置每次并发的温度值\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4,1.8,2]\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4, 2]\n",
    "    manual_temperatures = [0.2, 0.5, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 0.8, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 1, 2]\n",
    "    # manual_temperatures = [0.2, 0.8, 2] # 客服总结\n",
    "    # manual_temperatures = [0.2, 2] # topk = 1\n",
    "\n",
    "    \n",
    "    # 执行多视角分析，将所有参数都放在函数调用中\n",
    "    result = multi_perspective_analysis(\n",
    "        metaprompt=metaprompt,\n",
    "        p=len(manual_temperatures),  # 视角数量\n",
    "        topk=2,  # 返回前2个视角\n",
    "        temperature_settings=manual_temperatures,\n",
    "        complexity_weight_ratio=0.6,  # 复杂度权重占比\n",
    "        hidden_size=768,  # 隐藏层大小\n",
    "        length_weight=0.4,  # 文本长度复杂度权重\n",
    "        diversity_weight=0.5,  # 词汇多样性权重\n",
    "        variance_weight=0.3  # 语义嵌入方差权重\n",
    "    )\n",
    "\n",
    "    # 输出结果（格式化JSON）\n",
    "    print(\"\\n=== 多视角分析最终结果 ===\")\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import KafkaDsn\n",
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class HierarchicalAggregationLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, attn_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        分层聚合层：通过三级处理融合多视角特征\n",
    "        1) 两两视角组合的初级聚合\n",
    "        2) 所有组合结果的全局聚合 \n",
    "        3) 原始视角的注意力加权聚合\n",
    "        \n",
    "        参数：\n",
    "            hidden_size (int): 特征向量的维度（默认768）\n",
    "            num_perspectives (int): 需要聚合的视角数量（默认3）\n",
    "            attn_smoothing (float): 注意力权重的标签平滑系数（默认0.1）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.attn_smoothing = attn_smoothing\n",
    "        \n",
    "        # 第一级聚合器：处理所有视角的两两组合\n",
    "        # 共需要n*(n-1)/2个聚合器（n=视角数量）\n",
    "        self.pairwise_aggregators = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size * 2, hidden_size),  # 将两个视角拼接后线性变换\n",
    "                torch.nn.LayerNorm(hidden_size),               # 层归一化稳定训练\n",
    "                torch.nn.GELU()                                # 高斯误差线性单元激活\n",
    "            )\n",
    "            for _ in range(num_perspectives * (num_perspectives - 1) // 2)\n",
    "        ])\n",
    "        \n",
    "        # 第二级聚合器：整合所有两两组合的结果\n",
    "        num_pairwise = num_perspectives * (num_perspectives - 1) // 2\n",
    "        self.global_aggregator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_pairwise, hidden_size * 2),  # 扩大维度增强表达能力\n",
    "            torch.nn.LayerNorm(hidden_size * 2),                           # 归一化\n",
    "            torch.nn.GELU(),                                               # 非线性激活\n",
    "            torch.nn.Dropout(0.15),                                        # 随机失活防止过拟合\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size)                  # 降维到原始维度\n",
    "        )\n",
    "        \n",
    "        # 注意力机制：计算各原始视角的重要性权重\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),  # 压缩视角拼接信息\n",
    "            torch.nn.LayerNorm(hidden_size),                               # 归一化\n",
    "            torch.nn.GELU(),                                              # 激活函数\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)                 # 输出各视角权重\n",
    "        )\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # 将权重归一化为概率分布\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        \n",
    "        参数：\n",
    "            encoded_responses: 多个视角的特征列表，每个元素形状为[hidden_size]\n",
    "        \n",
    "        返回：\n",
    "            final_output: 聚合后的最终特征 [1, hidden_size]\n",
    "            weights: 各视角的注意力权重 [1, num_perspectives] \n",
    "        \"\"\"\n",
    "        # 将多个视角特征堆叠为张量 [num_perspectives, hidden_size]\n",
    "        stacked = torch.stack(encoded_responses)  \n",
    "        \n",
    "        # 第一阶段：两两视角聚合\n",
    "        pairwise_outputs = []\n",
    "        idx = 0  # 用于选择对应的聚合器\n",
    "        \n",
    "        # 遍历所有独特的视角组合对\n",
    "        for i in range(self.num_perspectives):\n",
    "            for j in range(i + 1, self.num_perspectives):\n",
    "                # 拼接两个视角特征 [1, hidden_size*2]\n",
    "                pair = torch.cat([stacked[i], stacked[j]], dim=-1).unsqueeze(0)\n",
    "                \n",
    "                # 通过对应的聚合器处理\n",
    "                pairwise_outputs.append(self.pairwise_aggregators[idx](pair))\n",
    "                idx += 1\n",
    "        \n",
    "        # 第二阶段：全局聚合\n",
    "        # 拼接所有两两聚合结果 [1, hidden_size*num_pairs]\n",
    "        pairwise_concat = torch.cat(pairwise_outputs, dim=-1)\n",
    "        \n",
    "        # 生成全局聚合特征 [1, hidden_size]\n",
    "        global_repr = self.global_aggregator(pairwise_concat)\n",
    "        \n",
    "        # 第三阶段：注意力权重计算\n",
    "        # 展平所有原始视角特征 [1, num_perspectives*hidden_size]\n",
    "        concat = stacked.view(1, -1)\n",
    "        \n",
    "        # 计算归一化注意力权重 [1, num_perspectives]\n",
    "        weights = self.softmax(self.attention(concat))\n",
    "        \n",
    "        # 应用标签平滑（正则化技术）\n",
    "        if self.attn_smoothing > 0:\n",
    "            uniform_dist = 1 / self.num_perspectives  # 均匀分布\n",
    "            weights = weights * (1 - self.attn_smoothing) + self.attn_smoothing * uniform_dist\n",
    "        \n",
    "        # 计算原始视角的加权和 [hidden_size]\n",
    "        weighted_aggregation = torch.sum(\n",
    "            stacked * weights.t().unsqueeze(-1),  # 加权\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        # 最终输出：全局特征 + 加权原始特征 [1, hidden_size]\n",
    "        final_output = global_repr + weighted_aggregation.unsqueeze(0)\n",
    "        \n",
    "        return final_output, weights\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"加载预训练的BERT编码器和分词器\"\"\"\n",
    "    model_name = \"D:\\\\xiangyue\\\\model\\\\bert-base-chinese\"\n",
    "    return AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    将文本编码为向量表示\n",
    "    使用BERT模型提取[CLS]标记作为文本的整体表示\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # 不计算梯度以提高推理效率\n",
    "        # 提取[CLS]标记的嵌入作为文本表示\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm, temperature=0.1):\n",
    "    \"\"\"\n",
    "    处理单个视角：\n",
    "    1. 调用大语言模型生成特定视角的回答\n",
    "    2. 将回答编码为向量表示\n",
    "    返回包含索引、回答文本和编码向量的元组\n",
    "    \n",
    "    参数:\n",
    "        index: 视角索引\n",
    "        question: 问题文本\n",
    "        encoder_model: 编码器模型\n",
    "        tokenizer: 分词器\n",
    "        llm: 大语言模型调用函数\n",
    "        temperature: 控制生成随机性的温度值\n",
    "    \"\"\"\n",
    "    # 使用特定视角前缀提示大语言模型，并传入temperature参数\n",
    "    response = llm(f\"【换一种视角思考{index+1}】{question}\", temperature=temperature)\n",
    "    return index, response, encode_text(response, encoder_model, tokenizer)\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1, llm=None, temperature_settings=None):\n",
    "    \"\"\"\n",
    "    多视角分析主函数：\n",
    "    1. 并行生成多个视角的回答\n",
    "    2. 对回答进行编码\n",
    "    3. 使用增强聚合层计算各视角权重\n",
    "    4. 选择权重最高的视角作为最终结果\n",
    "    \n",
    "    参数:\n",
    "        metaprompt: 输入的问题/提示\n",
    "        p: 视角数量\n",
    "        topk: 返回topk个最佳视角\n",
    "        llm: 大语言模型调用函数\n",
    "        temperature_settings: 温度值设置，可以是单个值或长度为p的列表\n",
    "    \"\"\"\n",
    "    # 如果未提供llm，则使用全局导入的llm_qwen\n",
    "    if llm is None:\n",
    "        llm = llm_qwen\n",
    "    \n",
    "    # 处理temperature设置\n",
    "    if temperature_settings is None:\n",
    "        temperatures = [0.1] * p  # 默认所有请求使用0.1\n",
    "    elif isinstance(temperature_settings, (int, float)):\n",
    "        temperatures = [temperature_settings] * p  # 单个值应用到所有请求\n",
    "    elif isinstance(temperature_settings, (list, tuple)) and len(temperature_settings) == p:\n",
    "        temperatures = temperature_settings  # 使用提供的温度列表\n",
    "    else:\n",
    "        raise ValueError(\"temperature_settings should be a single value or a list of length p\")\n",
    "    \n",
    "    # 初始化聚合模型和编码器\n",
    "    aggregation_model = HierarchicalAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # 使用线程池并行处理多个，每个请求使用不同的temperature\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, \n",
    "                i, \n",
    "                metaprompt, \n",
    "                encoder_model, \n",
    "                tokenizer, \n",
    "                llm, \n",
    "                temperatures[i]\n",
    "            ) for i in range(p)\n",
    "        ]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    # 按索引排序确保顺序正确\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    encoded_responses = [enc for _, _, enc in results]  # 提取编码向量\n",
    "    response_texts = [resp for _, resp, _ in results]  # 提取回答文本\n",
    "    for idx, (text, temp) in enumerate(zip(response_texts, temperatures), start=1):\n",
    "        print(f\"第{idx}个(temperature={temp:.2f})：{text}\")\n",
    "        \n",
    "    if encoded_responses:\n",
    "        # 聚合多视角信息并获取权重\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # 按权重排序视角\n",
    "        sorted_items = sorted([(i, f\"response_{i+1}\", weights[0, i].item()) for i in range(p)], \n",
    "                             key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        # 获取topk视角\n",
    "        top_indices = torch.topk(weights[0], topk).indices\n",
    "        top_perspectives = [(f\"response_{idx+1}\", weights[0, idx].item(), response_texts[idx]) \n",
    "                           for idx in top_indices]\n",
    "        \n",
    "        # 构建最终标题（当前已注释掉）\n",
    "        final_title = \"\\n\".join(re.sub(r'【[^】]+】', '', content).strip() for _, _, content in top_perspectives)\n",
    "        \n",
    "        return {\n",
    "            # \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            # \"temperature_settings\": temperatures,  # 返回使用的温度设置\n",
    "            # \"all_weights\": [(f\"response_{i+1}\", weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    metaprompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # 手动设置每次并发的温度值\n",
    "    manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4,1.8,2]\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.5, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 0.8, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 1, 2]\n",
    "    # manual_temperatures = [0.2, 0.8, 2] # 客服总结\n",
    "    # manual_temperatures = [0.2, 2] # topk = 1\n",
    "\n",
    "    result = multi_perspective_analysis(\n",
    "        metaprompt, \n",
    "        p=len(manual_temperatures),  # 自动根据温度列表长度确定p值\n",
    "        topk=2, \n",
    "        llm=llm_qwen, \n",
    "        temperature_settings=manual_temperatures  # 传入手动设置的温度列表\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))  # 以JSON格式输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "class EnhancedAggregationLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    增强型聚合层，用于多视角响应的语义聚合\n",
    "    融合ParScale技术的注意力平滑机制，提升多视角分析的稳定性\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, parscale_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        初始化聚合层\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: 编码器输出的隐藏层维度，默认768（BERT-base配置）\n",
    "            num_perspectives: 多视角分析的视角数量，默认3个视角\n",
    "            parscale_smoothing: ParScale注意力平滑系数，防止极端权重，默认0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.hidden_size = hidden_size\n",
    "        self.parscale_smoothing = parscale_smoothing\n",
    "        \n",
    "        # 定义聚合网络结构：线性变换+层归一化+激活函数+Dropout+输出权重\n",
    "        self.aggregate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.SiLU(),  # 平滑激活函数，替代ReLU\n",
    "            torch.nn.Dropout(0.1),  # 防止过拟合\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # 权重归一化\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        前向传播：计算多视角响应的聚合权重和综合表示\n",
    "        \n",
    "        Args:\n",
    "            encoded_responses: 各视角响应的编码向量列表，形状为[p, h]\n",
    "        \n",
    "        Returns:\n",
    "            aggregated_output: 聚合后的综合向量，形状为[h]\n",
    "            weights: 各视角的注意力权重，形状为[1, p]\n",
    "        \"\"\"\n",
    "        stacked_responses = torch.stack(encoded_responses)  # 堆叠视角编码为[p, h]\n",
    "        concat_responses = stacked_responses.view(1, -1)  # 拼接为[1, p*h]用于特征提取\n",
    "        \n",
    "        raw_weights = self.aggregate_layer(concat_responses)  # 计算原始权重\n",
    "        weights = self.softmax(raw_weights)  # 权重归一化\n",
    "        \n",
    "        # 应用ParScale注意力平滑：防止某个视角权重过大\n",
    "        if self.parscale_smoothing > 0:\n",
    "            uniform_weight = 1.0 / self.num_perspectives\n",
    "            weights = weights * (1 - self.parscale_smoothing) + self.parscale_smoothing * uniform_weight\n",
    "        \n",
    "        # 加权求和生成综合表示\n",
    "        weighted_sum = torch.sum(stacked_responses * weights.t(), dim=0)  # [p, h] * [p, 1] 后求和\n",
    "        return weighted_sum, weights\n",
    "\n",
    "\n",
    "# 初始化OpenAI客户端，连接到本地Qwen模型服务\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    调用Qwen模型API生成响应\n",
    "    \n",
    "    Args:\n",
    "        prompt: 输入提示词\n",
    "        model: 模型名称，默认Qwen3\n",
    "        temperature: 生成温度，控制随机性，默认0.7\n",
    "        top_p: 核采样参数，控制生成多样性，默认0.8\n",
    "        max_tokens: 最大生成长度，默认1024\n",
    "    \n",
    "    Returns:\n",
    "        模型生成的文本响应\n",
    "    \"\"\"\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"\n",
    "    加载文本编码器模型（BERT-base）用于语义向量化\n",
    "    \n",
    "    Returns:\n",
    "        model: BERT模型实例\n",
    "        tokenizer: 对应的分词器\n",
    "    \"\"\"\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    将文本转换为语义向量表示（使用BERT的[CLS]标记）\n",
    "    \n",
    "    Args:\n",
    "        text: 输入文本\n",
    "        model: BERT模型实例\n",
    "        tokenizer: 分词器实例\n",
    "    \n",
    "    Returns:\n",
    "        文本的语义编码向量，形状为[hidden_size]\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # 推理时关闭梯度计算\n",
    "        outputs = model(**inputs)\n",
    "    # 使用BERT的第一个标记（[CLS]）作为整体语义表示\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm_qwen):\n",
    "    \"\"\"\n",
    "    处理单个视角：生成响应并进行语义编码\n",
    "    \n",
    "    Args:\n",
    "        index: 视角索引（从0开始）\n",
    "        question: 分析问题\n",
    "        encoder_model: 编码器模型\n",
    "        tokenizer: 分词器\n",
    "        llm_qwen: LLM调用函数\n",
    "    \n",
    "    Returns:\n",
    "        index: 视角索引\n",
    "        response: 模型生成的响应文本\n",
    "        encoded_response: 响应的语义编码\n",
    "    \"\"\"\n",
    "    # 为每个视角添加标识，引导模型从不同角度思考\n",
    "    perspective_prompt = f\"【视角{index+1}】{question}\"\n",
    "    response = llm_qwen(perspective_prompt)  # 调用Qwen模型生成响应\n",
    "    encoded_response = encode_text(response, encoder_model, tokenizer)  # 编码响应文本\n",
    "    return index, response, encoded_response\n",
    "\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1):\n",
    "    \"\"\"\n",
    "    多视角分析主函数：生成多个视角响应并聚合分析结果\n",
    "    \n",
    "    Args:\n",
    "        metaprompt: 元提示词（分析问题）\n",
    "        p: 生成的视角数量，默认3个\n",
    "        topk: 保留的关键视角数量，默认1个\n",
    "    \n",
    "    Returns:\n",
    "        包含聚合结果、关键视角和所有视角权重的字典\n",
    "    \"\"\"\n",
    "    # 初始化聚合模型与编码器\n",
    "    aggregation_model = EnhancedAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # 并行处理多个视角（使用线程池提高效率）\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, i, metaprompt, encoder_model, tokenizer, llm_qwen\n",
    "            )\n",
    "            for i in range(p)\n",
    "        ]\n",
    "        # 收集所有视角的处理结果\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    # 按视角索引排序结果\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    # 提取响应文本和编码向量\n",
    "    responses = [(f\"response_{i+1}\", resp) for i, (_, resp, _) in enumerate(results)]\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # 打印原始响应（调试和可视化）\n",
    "    print(\"=== 原始响应 ===\")\n",
    "    for i, (key, response) in enumerate(responses):\n",
    "        print(f\"{key}: {response}\")\n",
    "    \n",
    "    # 执行聚合逻辑（仅当有有效响应时）\n",
    "    if encoded_responses:\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # 打印各视角的聚合权重\n",
    "        print(\"\\n=== 聚合权重 ===\")\n",
    "        for i, (key, _) in enumerate(responses):\n",
    "            print(f\"{key}: {weights[0, i].item():.4f}\")\n",
    "        \n",
    "        # 获取权重最高的topk个视角\n",
    "        top_weights, top_indices = torch.topk(weights[0], topk)\n",
    "        print(f\"\\n=== Top {topk} 视角 ===\")\n",
    "        top_perspectives = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            key = responses[idx][0]\n",
    "            weight = top_weights[i].item()\n",
    "            content = response_texts[idx]\n",
    "            print(f\"{key}: 权重={weight:.4f}\\n内容: {content}\")\n",
    "            top_perspectives.append((key, weight, content))\n",
    "        \n",
    "        # 生成最终聚合结果（合并关键视角内容）\n",
    "        final_title = \"\\n\".join(re.sub(r'【[^】]+】', '', content).strip() for _, _, content in top_perspectives)\n",
    "        print(\"\\n=== 最终聚合结果 ===\")\n",
    "        print(final_title)\n",
    "        \n",
    "        return {\n",
    "            \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            \"all_weights\": [(responses[i][0], weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    else:\n",
    "        print(\"没有有效的回答可供聚合\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例元提示词（空提示用于测试基本功能）\n",
    "    metaprompt = \"\"\"\n",
    "/no_think\n",
    "\"\"\"\n",
    "    # 执行多视角分析（生成3个视角，保留1个关键视角）\n",
    "    result = multi_perspective_analysis(metaprompt, p=3, topk=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
