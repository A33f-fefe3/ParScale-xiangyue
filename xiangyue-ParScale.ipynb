{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮你的吗？😊\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024, presence_penalty=1.5):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_output = llm_qwen(\"你好\")\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "class EnhancedAggregationLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    增强型聚合层，用于多视角响应的语义聚合\n",
    "    融合ParScale技术的注意力平滑机制，提升多视角分析的稳定性\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, parscale_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        初始化聚合层\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: 编码器输出的隐藏层维度，默认768（BERT-base配置）\n",
    "            num_perspectives: 多视角分析的视角数量，默认3个视角\n",
    "            parscale_smoothing: ParScale注意力平滑系数，防止极端权重，默认0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.hidden_size = hidden_size\n",
    "        self.parscale_smoothing = parscale_smoothing\n",
    "        \n",
    "        # 定义聚合网络结构：线性变换+层归一化+激活函数+Dropout+输出权重\n",
    "        self.aggregate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.SiLU(),  # 平滑激活函数，替代ReLU\n",
    "            torch.nn.Dropout(0.1),  # 防止过拟合\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # 权重归一化\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        前向传播：计算多视角响应的聚合权重和综合表示\n",
    "        \n",
    "        Args:\n",
    "            encoded_responses: 各视角响应的编码向量列表，形状为[p, h]\n",
    "        \n",
    "        Returns:\n",
    "            aggregated_output: 聚合后的综合向量，形状为[h]\n",
    "            weights: 各视角的注意力权重，形状为[1, p]\n",
    "        \"\"\"\n",
    "        stacked_responses = torch.stack(encoded_responses)  # 堆叠视角编码为[p, h]\n",
    "        concat_responses = stacked_responses.view(1, -1)  # 拼接为[1, p*h]用于特征提取\n",
    "        \n",
    "        raw_weights = self.aggregate_layer(concat_responses)  # 计算原始权重\n",
    "        weights = self.softmax(raw_weights)  # 权重归一化\n",
    "        \n",
    "        # 应用ParScale注意力平滑：防止某个视角权重过大\n",
    "        if self.parscale_smoothing > 0:\n",
    "            uniform_weight = 1.0 / self.num_perspectives\n",
    "            weights = weights * (1 - self.parscale_smoothing) + self.parscale_smoothing * uniform_weight\n",
    "        \n",
    "        # 加权求和生成综合表示\n",
    "        weighted_sum = torch.sum(stacked_responses * weights.t(), dim=0)  # [p, h] * [p, 1] 后求和\n",
    "        return weighted_sum, weights\n",
    "\n",
    "\n",
    "# 初始化OpenAI客户端，连接到本地Qwen模型服务\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    调用Qwen模型API生成响应\n",
    "    \n",
    "    Args:\n",
    "        prompt: 输入提示词\n",
    "        model: 模型名称，默认Qwen3\n",
    "        temperature: 生成温度，控制随机性，默认0.7\n",
    "        top_p: 核采样参数，控制生成多样性，默认0.8\n",
    "        max_tokens: 最大生成长度，默认1024\n",
    "    \n",
    "    Returns:\n",
    "        模型生成的文本响应\n",
    "    \"\"\"\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"\n",
    "    加载文本编码器模型（BERT-base）用于语义向量化\n",
    "    \n",
    "    Returns:\n",
    "        model: BERT模型实例\n",
    "        tokenizer: 对应的分词器\n",
    "    \"\"\"\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    将文本转换为语义向量表示（使用BERT的[CLS]标记）\n",
    "    \n",
    "    Args:\n",
    "        text: 输入文本\n",
    "        model: BERT模型实例\n",
    "        tokenizer: 分词器实例\n",
    "    \n",
    "    Returns:\n",
    "        文本的语义编码向量，形状为[hidden_size]\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # 推理时关闭梯度计算\n",
    "        outputs = model(**inputs)\n",
    "    # 使用BERT的第一个标记（[CLS]）作为整体语义表示\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm_qwen):\n",
    "    \"\"\"\n",
    "    处理单个视角：生成响应并进行语义编码\n",
    "    \n",
    "    Args:\n",
    "        index: 视角索引（从0开始）\n",
    "        question: 分析问题\n",
    "        encoder_model: 编码器模型\n",
    "        tokenizer: 分词器\n",
    "        llm_qwen: LLM调用函数\n",
    "    \n",
    "    Returns:\n",
    "        index: 视角索引\n",
    "        response: 模型生成的响应文本\n",
    "        encoded_response: 响应的语义编码\n",
    "    \"\"\"\n",
    "    # 为每个视角添加标识，引导模型从不同角度思考\n",
    "    perspective_prompt = f\"【视角{index+1}】{question}\"\n",
    "    response = llm_qwen(perspective_prompt)  # 调用Qwen模型生成响应\n",
    "    encoded_response = encode_text(response, encoder_model, tokenizer)  # 编码响应文本\n",
    "    return index, response, encoded_response\n",
    "\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1):\n",
    "    \"\"\"\n",
    "    多视角分析主函数：生成多个视角响应并聚合分析结果\n",
    "    \n",
    "    Args:\n",
    "        metaprompt: 元提示词（分析问题）\n",
    "        p: 生成的视角数量，默认3个\n",
    "        topk: 保留的关键视角数量，默认1个\n",
    "    \n",
    "    Returns:\n",
    "        包含聚合结果、关键视角和所有视角权重的字典\n",
    "    \"\"\"\n",
    "    # 初始化聚合模型与编码器\n",
    "    aggregation_model = EnhancedAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # 并行处理多个视角（使用线程池提高效率）\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, i, metaprompt, encoder_model, tokenizer, llm_qwen\n",
    "            )\n",
    "            for i in range(p)\n",
    "        ]\n",
    "        # 收集所有视角的处理结果\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    # 按视角索引排序结果\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    # 提取响应文本和编码向量\n",
    "    responses = [(f\"response_{i+1}\", resp) for i, (_, resp, _) in enumerate(results)]\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # 打印原始响应（调试和可视化）\n",
    "    print(\"=== 原始响应 ===\")\n",
    "    for i, (key, response) in enumerate(responses):\n",
    "        print(f\"{key}: {response}\")\n",
    "    \n",
    "    # 执行聚合逻辑（仅当有有效响应时）\n",
    "    if encoded_responses:\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # 打印各视角的聚合权重\n",
    "        print(\"\\n=== 聚合权重 ===\")\n",
    "        for i, (key, _) in enumerate(responses):\n",
    "            print(f\"{key}: {weights[0, i].item():.4f}\")\n",
    "        \n",
    "        # 获取权重最高的topk个视角\n",
    "        top_weights, top_indices = torch.topk(weights[0], topk)\n",
    "        print(f\"\\n=== Top {topk} 视角 ===\")\n",
    "        top_perspectives = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            key = responses[idx][0]\n",
    "            weight = top_weights[i].item()\n",
    "            content = response_texts[idx]\n",
    "            print(f\"{key}: 权重={weight:.4f}\\n内容: {content}\")\n",
    "            top_perspectives.append((key, weight, content))\n",
    "        \n",
    "        # 生成最终聚合结果（合并关键视角内容）\n",
    "        final_title = \"\\n\".join(re.sub(r'【[^】]+】', '', content).strip() for _, _, content in top_perspectives)\n",
    "        print(\"\\n=== 最终聚合结果 ===\")\n",
    "        print(final_title)\n",
    "        \n",
    "        return {\n",
    "            \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            \"all_weights\": [(responses[i][0], weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    else:\n",
    "        print(\"没有有效的回答可供聚合\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例元提示词（空提示用于测试基本功能）\n",
    "    metaprompt = \"\"\"\n",
    "/no_think\n",
    "\"\"\"\n",
    "    # 执行多视角分析（生成3个视角，保留1个关键视角）\n",
    "    result = multi_perspective_analysis(metaprompt, p=3, topk=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
