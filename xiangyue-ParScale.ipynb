{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024, presence_penalty=1.5):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_output = llm_qwen(\"ä½ å¥½\")\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "class EnhancedAggregationLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    å¢å¼ºå‹èšåˆå±‚ï¼Œç”¨äºå¤šè§†è§’å“åº”çš„è¯­ä¹‰èšåˆ\n",
    "    èåˆParScaleæŠ€æœ¯çš„æ³¨æ„åŠ›å¹³æ»‘æœºåˆ¶ï¼Œæå‡å¤šè§†è§’åˆ†æçš„ç¨³å®šæ€§\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, parscale_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–èšåˆå±‚\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: ç¼–ç å™¨è¾“å‡ºçš„éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤768ï¼ˆBERT-baseé…ç½®ï¼‰\n",
    "            num_perspectives: å¤šè§†è§’åˆ†æçš„è§†è§’æ•°é‡ï¼Œé»˜è®¤3ä¸ªè§†è§’\n",
    "            parscale_smoothing: ParScaleæ³¨æ„åŠ›å¹³æ»‘ç³»æ•°ï¼Œé˜²æ­¢æç«¯æƒé‡ï¼Œé»˜è®¤0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.hidden_size = hidden_size\n",
    "        self.parscale_smoothing = parscale_smoothing\n",
    "        \n",
    "        # å®šä¹‰èšåˆç½‘ç»œç»“æ„ï¼šçº¿æ€§å˜æ¢+å±‚å½’ä¸€åŒ–+æ¿€æ´»å‡½æ•°+Dropout+è¾“å‡ºæƒé‡\n",
    "        self.aggregate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.SiLU(),  # å¹³æ»‘æ¿€æ´»å‡½æ•°ï¼Œæ›¿ä»£ReLU\n",
    "            torch.nn.Dropout(0.1),  # é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # æƒé‡å½’ä¸€åŒ–\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ï¼šè®¡ç®—å¤šè§†è§’å“åº”çš„èšåˆæƒé‡å’Œç»¼åˆè¡¨ç¤º\n",
    "        \n",
    "        Args:\n",
    "            encoded_responses: å„è§†è§’å“åº”çš„ç¼–ç å‘é‡åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º[p, h]\n",
    "        \n",
    "        Returns:\n",
    "            aggregated_output: èšåˆåçš„ç»¼åˆå‘é‡ï¼Œå½¢çŠ¶ä¸º[h]\n",
    "            weights: å„è§†è§’çš„æ³¨æ„åŠ›æƒé‡ï¼Œå½¢çŠ¶ä¸º[1, p]\n",
    "        \"\"\"\n",
    "        stacked_responses = torch.stack(encoded_responses)  # å †å è§†è§’ç¼–ç ä¸º[p, h]\n",
    "        concat_responses = stacked_responses.view(1, -1)  # æ‹¼æ¥ä¸º[1, p*h]ç”¨äºç‰¹å¾æå–\n",
    "        \n",
    "        raw_weights = self.aggregate_layer(concat_responses)  # è®¡ç®—åŸå§‹æƒé‡\n",
    "        weights = self.softmax(raw_weights)  # æƒé‡å½’ä¸€åŒ–\n",
    "        \n",
    "        # åº”ç”¨ParScaleæ³¨æ„åŠ›å¹³æ»‘ï¼šé˜²æ­¢æŸä¸ªè§†è§’æƒé‡è¿‡å¤§\n",
    "        if self.parscale_smoothing > 0:\n",
    "            uniform_weight = 1.0 / self.num_perspectives\n",
    "            weights = weights * (1 - self.parscale_smoothing) + self.parscale_smoothing * uniform_weight\n",
    "        \n",
    "        # åŠ æƒæ±‚å’Œç”Ÿæˆç»¼åˆè¡¨ç¤º\n",
    "        weighted_sum = torch.sum(stacked_responses * weights.t(), dim=0)  # [p, h] * [p, 1] åæ±‚å’Œ\n",
    "        return weighted_sum, weights\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°æœ¬åœ°Qwenæ¨¡å‹æœåŠ¡\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    è°ƒç”¨Qwenæ¨¡å‹APIç”Ÿæˆå“åº”\n",
    "    \n",
    "    Args:\n",
    "        prompt: è¾“å…¥æç¤ºè¯\n",
    "        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤Qwen3\n",
    "        temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤0.7\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼Œé»˜è®¤0.8\n",
    "        max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œé»˜è®¤1024\n",
    "    \n",
    "    Returns:\n",
    "        æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å“åº”\n",
    "    \"\"\"\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"\n",
    "    åŠ è½½æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ï¼ˆBERT-baseï¼‰ç”¨äºè¯­ä¹‰å‘é‡åŒ–\n",
    "    \n",
    "    Returns:\n",
    "        model: BERTæ¨¡å‹å®ä¾‹\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨\n",
    "    \"\"\"\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡è¡¨ç¤ºï¼ˆä½¿ç”¨BERTçš„[CLS]æ ‡è®°ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        text: è¾“å…¥æ–‡æœ¬\n",
    "        model: BERTæ¨¡å‹å®ä¾‹\n",
    "        tokenizer: åˆ†è¯å™¨å®ä¾‹\n",
    "    \n",
    "    Returns:\n",
    "        æ–‡æœ¬çš„è¯­ä¹‰ç¼–ç å‘é‡ï¼Œå½¢çŠ¶ä¸º[hidden_size]\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—\n",
    "        outputs = model(**inputs)\n",
    "    # ä½¿ç”¨BERTçš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰ä½œä¸ºæ•´ä½“è¯­ä¹‰è¡¨ç¤º\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm_qwen):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªè§†è§’ï¼šç”Ÿæˆå“åº”å¹¶è¿›è¡Œè¯­ä¹‰ç¼–ç \n",
    "    \n",
    "    Args:\n",
    "        index: è§†è§’ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰\n",
    "        question: åˆ†æé—®é¢˜\n",
    "        encoder_model: ç¼–ç å™¨æ¨¡å‹\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        llm_qwen: LLMè°ƒç”¨å‡½æ•°\n",
    "    \n",
    "    Returns:\n",
    "        index: è§†è§’ç´¢å¼•\n",
    "        response: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "        encoded_response: å“åº”çš„è¯­ä¹‰ç¼–ç \n",
    "    \"\"\"\n",
    "    # ä¸ºæ¯ä¸ªè§†è§’æ·»åŠ æ ‡è¯†ï¼Œå¼•å¯¼æ¨¡å‹ä»ä¸åŒè§’åº¦æ€è€ƒ\n",
    "    perspective_prompt = f\"ã€è§†è§’{index+1}ã€‘{question}\"\n",
    "    response = llm_qwen(perspective_prompt)  # è°ƒç”¨Qwenæ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    encoded_response = encode_text(response, encoder_model, tokenizer)  # ç¼–ç å“åº”æ–‡æœ¬\n",
    "    return index, response, encoded_response\n",
    "\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1):\n",
    "    \"\"\"\n",
    "    å¤šè§†è§’åˆ†æä¸»å‡½æ•°ï¼šç”Ÿæˆå¤šä¸ªè§†è§’å“åº”å¹¶èšåˆåˆ†æç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        metaprompt: å…ƒæç¤ºè¯ï¼ˆåˆ†æé—®é¢˜ï¼‰\n",
    "        p: ç”Ÿæˆçš„è§†è§’æ•°é‡ï¼Œé»˜è®¤3ä¸ª\n",
    "        topk: ä¿ç•™çš„å…³é”®è§†è§’æ•°é‡ï¼Œé»˜è®¤1ä¸ª\n",
    "    \n",
    "    Returns:\n",
    "        åŒ…å«èšåˆç»“æœã€å…³é”®è§†è§’å’Œæ‰€æœ‰è§†è§’æƒé‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–èšåˆæ¨¡å‹ä¸ç¼–ç å™¨\n",
    "    aggregation_model = EnhancedAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # å¹¶è¡Œå¤„ç†å¤šä¸ªè§†è§’ï¼ˆä½¿ç”¨çº¿ç¨‹æ± æé«˜æ•ˆç‡ï¼‰\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, i, metaprompt, encoder_model, tokenizer, llm_qwen\n",
    "            )\n",
    "            for i in range(p)\n",
    "        ]\n",
    "        # æ”¶é›†æ‰€æœ‰è§†è§’çš„å¤„ç†ç»“æœ\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    # æŒ‰è§†è§’ç´¢å¼•æ’åºç»“æœ\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    # æå–å“åº”æ–‡æœ¬å’Œç¼–ç å‘é‡\n",
    "    responses = [(f\"response_{i+1}\", resp) for i, (_, resp, _) in enumerate(results)]\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # æ‰“å°åŸå§‹å“åº”ï¼ˆè°ƒè¯•å’Œå¯è§†åŒ–ï¼‰\n",
    "    print(\"=== åŸå§‹å“åº” ===\")\n",
    "    for i, (key, response) in enumerate(responses):\n",
    "        print(f\"{key}: {response}\")\n",
    "    \n",
    "    # æ‰§è¡Œèšåˆé€»è¾‘ï¼ˆä»…å½“æœ‰æœ‰æ•ˆå“åº”æ—¶ï¼‰\n",
    "    if encoded_responses:\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # æ‰“å°å„è§†è§’çš„èšåˆæƒé‡\n",
    "        print(\"\\n=== èšåˆæƒé‡ ===\")\n",
    "        for i, (key, _) in enumerate(responses):\n",
    "            print(f\"{key}: {weights[0, i].item():.4f}\")\n",
    "        \n",
    "        # è·å–æƒé‡æœ€é«˜çš„topkä¸ªè§†è§’\n",
    "        top_weights, top_indices = torch.topk(weights[0], topk)\n",
    "        print(f\"\\n=== Top {topk} è§†è§’ ===\")\n",
    "        top_perspectives = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            key = responses[idx][0]\n",
    "            weight = top_weights[i].item()\n",
    "            content = response_texts[idx]\n",
    "            print(f\"{key}: æƒé‡={weight:.4f}\\nå†…å®¹: {content}\")\n",
    "            top_perspectives.append((key, weight, content))\n",
    "        \n",
    "        # ç”Ÿæˆæœ€ç»ˆèšåˆç»“æœï¼ˆåˆå¹¶å…³é”®è§†è§’å†…å®¹ï¼‰\n",
    "        final_title = \"\\n\".join(re.sub(r'ã€[^ã€‘]+ã€‘', '', content).strip() for _, _, content in top_perspectives)\n",
    "        print(\"\\n=== æœ€ç»ˆèšåˆç»“æœ ===\")\n",
    "        print(final_title)\n",
    "        \n",
    "        return {\n",
    "            \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            \"all_weights\": [(responses[i][0], weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æœ‰æ•ˆçš„å›ç­”å¯ä¾›èšåˆ\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ç¤ºä¾‹å…ƒæç¤ºè¯ï¼ˆç©ºæç¤ºç”¨äºæµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼‰\n",
    "    metaprompt = \"\"\"\n",
    "/no_think\n",
    "\"\"\"\n",
    "    # æ‰§è¡Œå¤šè§†è§’åˆ†æï¼ˆç”Ÿæˆ3ä¸ªè§†è§’ï¼Œä¿ç•™1ä¸ªå…³é”®è§†è§’ï¼‰\n",
    "    result = multi_perspective_analysis(metaprompt, p=3, topk=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
