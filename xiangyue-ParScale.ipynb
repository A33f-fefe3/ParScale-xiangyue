{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ff29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=2048, presence_penalty=1.5):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        presence_penalty=presence_penalty,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_output = llm_qwen(\"ä½ å¥½\")\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6777fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Optional, Union\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# ä¸‹è½½NLTKæ‰€éœ€èµ„æºï¼ˆé¦–æ¬¡è¿è¡Œæ—¶å–æ¶ˆæ³¨é‡Šï¼‰\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "class ComplexityWeightedAggregationLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, complexity_weight_ratio=0.7):\n",
    "        \"\"\"\n",
    "        åŸºäºå¤æ‚åº¦åŠ æƒçš„èšåˆå±‚ï¼š\n",
    "        - ä¿ç•™å…¨å±€èšåˆè·¯å¾„\n",
    "        - ç”¨æ–‡æœ¬å¤æ‚åº¦æ›¿ä»£æ³¨æ„åŠ›æƒé‡\n",
    "        - complexity_weight_ratioæ§åˆ¶å¤æ‚åº¦æƒé‡å½±å“ç¨‹åº¦\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.complexity_weight_ratio = complexity_weight_ratio\n",
    "        \n",
    "        # ç¬¬ä¸€çº§ï¼šä¸¤ä¸¤è§†è§’èšåˆ\n",
    "        self.pairwise_aggregators = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size * 2, hidden_size),\n",
    "                torch.nn.LayerNorm(hidden_size),\n",
    "                torch.nn.GELU()\n",
    "            )\n",
    "            for _ in range(num_perspectives * (num_perspectives - 1) // 2)\n",
    "        ])\n",
    "        \n",
    "        # ç¬¬äºŒçº§ï¼šå…¨å±€èšåˆ\n",
    "        num_pairwise = num_perspectives * (num_perspectives - 1) // 2\n",
    "        self.global_aggregator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_pairwise, hidden_size * 2),\n",
    "            torch.nn.LayerNorm(hidden_size * 2),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.15),\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        # æ–‡æœ¬å¤æ‚åº¦è®¡ç®—æ¨¡å‹ï¼ˆå¤–éƒ¨ä¼ å…¥ï¼‰\n",
    "        self.complexity_model = None\n",
    "        self.complexity_tokenizer = None\n",
    "    \n",
    "    def set_complexity_tools(self, model: AutoModel, tokenizer: AutoTokenizer):\n",
    "        \"\"\"è®¾ç½®å¤æ‚åº¦è®¡ç®—æ‰€éœ€çš„æ¨¡å‹å’Œåˆ†è¯å™¨\"\"\"\n",
    "        self.complexity_model = model\n",
    "        self.complexity_tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self, encoded_responses: List[torch.Tensor], response_texts: List[str]):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            encoded_responses: è§†è§’ç‰¹å¾åˆ—è¡¨ [num_perspectives, hidden_size]\n",
    "            response_texts: è§†è§’æ–‡æœ¬åˆ—è¡¨ [num_perspectives]\n",
    "        \"\"\"\n",
    "        stacked = torch.stack(encoded_responses)  # [num_perspectives, hidden_size]\n",
    "        \n",
    "        # ç¬¬ä¸€é˜¶æ®µï¼šä¸¤ä¸¤è§†è§’èšåˆ\n",
    "        pairwise_outputs = []\n",
    "        idx = 0\n",
    "        for i in range(self.num_perspectives):\n",
    "            for j in range(i + 1, self.num_perspectives):\n",
    "                pair = torch.cat([stacked[i], stacked[j]], dim=-1).unsqueeze(0)\n",
    "                pairwise_outputs.append(self.pairwise_aggregators[idx](pair))\n",
    "                idx += 1\n",
    "        \n",
    "        # ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆ\n",
    "        pairwise_concat = torch.cat(pairwise_outputs, dim=-1)  # [1, hidden_size*num_pairs]\n",
    "        global_repr = self.global_aggregator(pairwise_concat)  # [1, hidden_size]\n",
    "        \n",
    "        # ç¬¬ä¸‰é˜¶æ®µï¼šåŸºäºå¤æ‚åº¦çš„æƒé‡è®¡ç®—\n",
    "        complexities = [\n",
    "            calculate_complexity(text, self.complexity_model, self.complexity_tokenizer)\n",
    "            for text in response_texts\n",
    "        ]\n",
    "        \n",
    "        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "        complexities = torch.tensor(complexities, dtype=torch.float32)\n",
    "        weights = torch.softmax(complexities, dim=-1)  # [num_perspectives]\n",
    "        \n",
    "        # åŸå§‹è§†è§’çš„åŠ æƒå’Œ\n",
    "        weighted_aggregation = torch.sum(\n",
    "            stacked * weights.t().unsqueeze(-1),\n",
    "            dim=0\n",
    "        )  # [hidden_size]\n",
    "        \n",
    "        # æœ€ç»ˆè¾“å‡ºèåˆ\n",
    "        final_output = (self.complexity_weight_ratio * weighted_aggregation.unsqueeze(0) + \n",
    "                       (1 - self.complexity_weight_ratio) * global_repr)  # [1, hidden_size]\n",
    "        \n",
    "        return final_output, weights\n",
    "\n",
    "def load_encoder_model() -> Tuple[AutoModel, AutoTokenizer]:\n",
    "    \"\"\"åŠ è½½é¢„è®­ç»ƒçš„BERTç¼–ç å™¨å’Œåˆ†è¯å™¨\"\"\"\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    return AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_text(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> torch.Tensor:\n",
    "    \"\"\"å°†æ–‡æœ¬ç¼–ç ä¸ºBERTçš„CLSå‘é‡\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze(0)  # [hidden_size]\n",
    "\n",
    "def calculate_complexity(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ–‡æœ¬å¤æ‚åº¦ï¼ˆå¤šç»´åº¦æŒ‡æ ‡èåˆï¼‰ï¼š\n",
    "    1. æ–‡æœ¬é•¿åº¦å¤æ‚åº¦\n",
    "    2. è¯æ±‡å¤šæ ·æ€§ï¼ˆTF-IDFç†µï¼‰\n",
    "    3. è¯­ä¹‰åµŒå…¥æ–¹å·®\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    \n",
    "    # 1. æ–‡æœ¬é•¿åº¦å¤æ‚åº¦ï¼ˆå¯¹æ•°å½’ä¸€åŒ–ï¼‰\n",
    "    word_count = len(word_tokenize(text))\n",
    "    length_score = np.log(word_count + 1) / np.log(100 + 1)  # å½’ä¸€åŒ–åˆ°[0,1]\n",
    "    \n",
    "    # 2. è¯æ±‡å¤šæ ·æ€§ï¼ˆTF-IDFç†µï¼‰\n",
    "    words = [word for word in word_tokenize(text) if word.isalpha() and len(word) > 1]\n",
    "    if len(words) < 2:\n",
    "        diversity_score = 0.0\n",
    "    else:\n",
    "        word_freq = Counter(words)\n",
    "        probs = np.array(list(word_freq.values()), dtype=float) / len(words)\n",
    "        diversity_score = -np.sum(probs * np.log(probs + 1e-10)) / np.log(len(word_freq))\n",
    "        diversity_score = max(0.0, min(1.0, diversity_score))  # è£å‰ªåˆ°[0,1]\n",
    "    \n",
    "    # 3. è¯­ä¹‰åµŒå…¥æ–¹å·®ï¼ˆBERTå„tokenå‘é‡çš„æ–¹å·®ï¼‰\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.squeeze(0)\n",
    "        valid_mask = inputs.input_ids.squeeze(0) != 0  # å¿½ç•¥padding\n",
    "        valid_embeddings = embeddings[valid_mask]\n",
    "        if len(valid_embeddings) < 2:\n",
    "            semantic_variance = 0.0\n",
    "        else:\n",
    "            variance = torch.var(valid_embeddings, dim=0).mean().item()\n",
    "            # åŠ¨æ€èŒƒå›´å½’ä¸€åŒ–ï¼ˆå‡è®¾æ–¹å·®é€šå¸¸åœ¨0.01-1.0ä¹‹é—´ï¼‰\n",
    "            semantic_variance = (variance - 0.01) / (1.0 - 0.01) if variance > 0.01 else 0.0\n",
    "            semantic_variance = max(0.0, min(1.0, semantic_variance))  # è£å‰ªåˆ°[0,1]\n",
    "    \n",
    "    # ç»¼åˆå¤æ‚åº¦åˆ†æ•°ï¼ˆå„æŒ‡æ ‡åŠ æƒæ±‚å’Œï¼‰\n",
    "    complexity_score = 0.3 * length_score + 0.3 * diversity_score + 0.4 * semantic_variance\n",
    "    return max(complexity_score, 1e-6)  # é˜²æ­¢å…¨é›¶\n",
    "\n",
    "def process_perspective(\n",
    "    index: int, \n",
    "    question: str, \n",
    "    encoder_model: AutoModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    llm: callable, \n",
    "    temperature: float\n",
    ") -> Tuple[int, str, torch.Tensor]:\n",
    "    \"\"\"å¤„ç†å•ä¸ªè§†è§’ï¼šç”Ÿæˆå›ç­” -> ç¼–ç å‘é‡\"\"\"\n",
    "    response = llm(f\"{question}\", temperature=temperature)\n",
    "    encoded = encode_text(response, encoder_model, tokenizer)\n",
    "    return index, response, encoded\n",
    "\n",
    "def multi_perspective_analysis(\n",
    "    metaprompt: str, \n",
    "    p: int = 3, \n",
    "    topk: int = 1, \n",
    "    llm: callable = None, \n",
    "    temperature_settings: List[float] = None,\n",
    "    complexity_weight_ratio: float = 0.4,\n",
    "    hidden_size: int = 768,\n",
    "    length_weight: float = 0.3,\n",
    "    diversity_weight: float = 0.3,\n",
    "    variance_weight: float = 0.4,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    å¤šè§†è§’åˆ†æä¸»å‡½æ•°ï¼š\n",
    "    1. å¹¶è¡Œç”Ÿæˆpä¸ªè§†è§’å›ç­”\n",
    "    2. è®¡ç®—å„è§†è§’å¤æ‚åº¦æƒé‡\n",
    "    3. èšåˆç‰¹å¾å¹¶è¿”å›topkè§†è§’\n",
    "    \n",
    "    å‚æ•°:\n",
    "        metaprompt: å…ƒæç¤ºæ–‡æœ¬\n",
    "        p: è§†è§’æ•°é‡\n",
    "        topk: è¿”å›çš„é¡¶éƒ¨è§†è§’æ•°é‡\n",
    "        llm: å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨å‡½æ•°\n",
    "        temperature_settings: å„è§†è§’çš„æ¸©åº¦è®¾ç½®\n",
    "        complexity_weight_ratio: å¤æ‚åº¦æƒé‡å æ¯”\n",
    "        hidden_size: éšè—å±‚å¤§å°\n",
    "        length_weight: æ–‡æœ¬é•¿åº¦å¤æ‚åº¦æƒé‡\n",
    "        diversity_weight: è¯æ±‡å¤šæ ·æ€§æƒé‡\n",
    "        variance_weight: è¯­ä¹‰åµŒå…¥æ–¹å·®æƒé‡\n",
    "    \"\"\"\n",
    "    global calculate_complexity\n",
    "    \n",
    "    if llm is None:\n",
    "        llm = llm_qwen  # é»˜è®¤ä½¿ç”¨æ¨¡æ‹ŸLLM\n",
    "    \n",
    "    # å¤„ç†temperatureè®¾ç½®\n",
    "    if temperature_settings is None:\n",
    "        temperatures = [0.1] * p\n",
    "    elif isinstance(temperature_settings, (int, float)):\n",
    "        temperatures = [temperature_settings] * p\n",
    "    elif len(temperature_settings) == p:\n",
    "        temperatures = temperature_settings\n",
    "    else:\n",
    "        raise ValueError(\"temperature_settingséœ€ä¸ºå•ä¸ªå€¼æˆ–é•¿åº¦ä¸ºpçš„åˆ—è¡¨\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # åˆ›å»ºé—­åŒ…ä»¥ä¿®æ”¹å¤æ‚åº¦è®¡ç®—çš„æƒé‡\n",
    "    original_calculate_complexity = calculate_complexity\n",
    "    def calculate_complexity_with_weights(text: str, model: AutoModel, tokenizer: AutoTokenizer) -> float:\n",
    "        \"\"\"ä½¿ç”¨è‡ªå®šä¹‰æƒé‡çš„æ–‡æœ¬å¤æ‚åº¦è®¡ç®—å‡½æ•°\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        # 1. æ–‡æœ¬é•¿åº¦å¤æ‚åº¦\n",
    "        word_count = len(word_tokenize(text))\n",
    "        length_score = np.log(word_count + 1) / np.log(100 + 1)\n",
    "        \n",
    "        # 2. è¯æ±‡å¤šæ ·æ€§\n",
    "        words = [word for word in word_tokenize(text) if word.isalpha() and len(word) > 1]\n",
    "        if len(words) < 2:\n",
    "            diversity_score = 0.0\n",
    "        else:\n",
    "            word_freq = Counter(words)\n",
    "            probs = np.array(list(word_freq.values()), dtype=float) / len(words)\n",
    "            diversity_score = -np.sum(probs * np.log(probs + 1e-10)) / np.log(len(word_freq))\n",
    "            diversity_score = max(0.0, min(1.0, diversity_score))\n",
    "        \n",
    "        # 3. è¯­ä¹‰åµŒå…¥æ–¹å·®\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**inputs).last_hidden_state.squeeze(0)\n",
    "            valid_mask = inputs.input_ids.squeeze(0) != 0\n",
    "            valid_embeddings = embeddings[valid_mask]\n",
    "            if len(valid_embeddings) < 2:\n",
    "                semantic_variance = 0.0\n",
    "            else:\n",
    "                variance = torch.var(valid_embeddings, dim=0).mean().item()\n",
    "                semantic_variance = (variance - 0.01) / (1.0 - 0.01) if variance > 0.01 else 0.0\n",
    "                semantic_variance = max(0.0, min(1.0, semantic_variance))\n",
    "        \n",
    "        # ä½¿ç”¨ä¼ å…¥çš„æƒé‡\n",
    "        complexity_score = (length_weight * length_score + \n",
    "                           diversity_weight * diversity_score + \n",
    "                           variance_weight * semantic_variance)\n",
    "        return max(complexity_score, 1e-6)\n",
    "    \n",
    "    # ä¸´æ—¶æ›¿æ¢å¤æ‚åº¦è®¡ç®—å‡½æ•°\n",
    "    calculate_complexity = calculate_complexity_with_weights\n",
    "    \n",
    "    aggregation_model = ComplexityWeightedAggregationLayer(\n",
    "        num_perspectives=p, \n",
    "        complexity_weight_ratio=complexity_weight_ratio,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "    aggregation_model.set_complexity_tools(encoder_model, tokenizer)\n",
    "    aggregation_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    \n",
    "    # å¹¶è¡Œå¤„ç†å¤šè§†è§’\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, \n",
    "                i, \n",
    "                metaprompt, \n",
    "                encoder_model, \n",
    "                tokenizer, \n",
    "                llm, \n",
    "                temperatures[i]\n",
    "            ) for i in range(p)\n",
    "        ]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    # æ¢å¤åŸå§‹çš„å¤æ‚åº¦è®¡ç®—å‡½æ•°\n",
    "    calculate_complexity = original_calculate_complexity\n",
    "    \n",
    "    # æ•´ç†ç»“æœ\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # æ‰“å°å„è§†è§’ä¿¡æ¯ï¼ˆå«å¤æ‚åº¦ï¼‰\n",
    "    print(\"\\n=== å„è§†è§’åˆ†æç»“æœ ===\")\n",
    "    complexities = [\n",
    "        calculate_complexity_with_weights(text, encoder_model, tokenizer)\n",
    "        for text in response_texts\n",
    "    ]\n",
    "    for idx, (text, temp, comp) in enumerate(zip(response_texts, temperatures, complexities), start=1):\n",
    "        # print(f\"è§†è§’{idx} (temp={temp:.2f}, å¤æ‚åº¦={comp:.4f}):\")\n",
    "        print(f\"è§†è§’{idx} temp={temp:.2f}\")\n",
    "        print(f\"  {text}\\n\")\n",
    "    \n",
    "    if not encoded_responses:\n",
    "        return {\"error\": \"æœªè·å–åˆ°æœ‰æ•ˆè§†è§’\"}\n",
    "    \n",
    "    # æ‰§è¡Œå¤æ‚åº¦åŠ æƒèšåˆ\n",
    "    with torch.no_grad():\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses, response_texts)\n",
    "    \n",
    "    # æ•´ç†topkç»“æœ\n",
    "    sorted_items = sorted(\n",
    "        [(i, f\"è§†è§’{i+1}\", weights[i].item(), complexities[i]) for i in range(p)],\n",
    "        key=lambda x: x[2], reverse=True\n",
    "    )\n",
    "    top_indices = [item[0] for item in sorted_items[:topk]]\n",
    "    top_perspectives = [\n",
    "        {\n",
    "            \"è§†è§’\": f\"è§†è§’{idx+1}\",\n",
    "            \"æƒé‡\": weights[idx].item(),\n",
    "            \"å¤æ‚åº¦\": complexities[idx],\n",
    "            \"å›ç­”\": response_texts[idx]\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"æ€»è§†è§’æ•°\": p,\n",
    "        \"topkç»“æœ\": top_perspectives,\n",
    "        \"æ‰€æœ‰è§†è§’æƒé‡\": [(item[1], item[2]) for item in sorted_items],\n",
    "        \"å¤æ‚åº¦æŒ‡æ ‡\": [(item[1], item[3]) for item in sorted_items],\n",
    "        \"å‚æ•°é…ç½®\": {\n",
    "            \"complexity_weight_ratio\": complexity_weight_ratio,\n",
    "            \"length_weight\": length_weight,\n",
    "            \"diversity_weight\": diversity_weight,\n",
    "            \"variance_weight\": variance_weight,\n",
    "            \"temperature_settings\": temperatures\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    metaprompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # æ‰‹åŠ¨è®¾ç½®æ¯æ¬¡å¹¶å‘çš„æ¸©åº¦å€¼\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4,1.8,2]\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4, 2]\n",
    "    manual_temperatures = [0.2, 0.5, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 0.8, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 1, 2]\n",
    "    # manual_temperatures = [0.2, 0.8, 2] # å®¢æœæ€»ç»“\n",
    "    # manual_temperatures = [0.2, 2] # topk = 1\n",
    "\n",
    "    \n",
    "    # æ‰§è¡Œå¤šè§†è§’åˆ†æï¼Œå°†æ‰€æœ‰å‚æ•°éƒ½æ”¾åœ¨å‡½æ•°è°ƒç”¨ä¸­\n",
    "    result = multi_perspective_analysis(\n",
    "        metaprompt=metaprompt,\n",
    "        p=len(manual_temperatures),  # è§†è§’æ•°é‡\n",
    "        topk=2,  # è¿”å›å‰2ä¸ªè§†è§’\n",
    "        temperature_settings=manual_temperatures,\n",
    "        complexity_weight_ratio=0.6,  # å¤æ‚åº¦æƒé‡å æ¯”\n",
    "        hidden_size=768,  # éšè—å±‚å¤§å°\n",
    "        length_weight=0.4,  # æ–‡æœ¬é•¿åº¦å¤æ‚åº¦æƒé‡\n",
    "        diversity_weight=0.5,  # è¯æ±‡å¤šæ ·æ€§æƒé‡\n",
    "        variance_weight=0.3  # è¯­ä¹‰åµŒå…¥æ–¹å·®æƒé‡\n",
    "    )\n",
    "\n",
    "    # è¾“å‡ºç»“æœï¼ˆæ ¼å¼åŒ–JSONï¼‰\n",
    "    print(\"\\n=== å¤šè§†è§’åˆ†ææœ€ç»ˆç»“æœ ===\")\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import KafkaDsn\n",
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class HierarchicalAggregationLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, attn_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        åˆ†å±‚èšåˆå±‚ï¼šé€šè¿‡ä¸‰çº§å¤„ç†èåˆå¤šè§†è§’ç‰¹å¾\n",
    "        1) ä¸¤ä¸¤è§†è§’ç»„åˆçš„åˆçº§èšåˆ\n",
    "        2) æ‰€æœ‰ç»„åˆç»“æœçš„å…¨å±€èšåˆ \n",
    "        3) åŸå§‹è§†è§’çš„æ³¨æ„åŠ›åŠ æƒèšåˆ\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            hidden_size (int): ç‰¹å¾å‘é‡çš„ç»´åº¦ï¼ˆé»˜è®¤768ï¼‰\n",
    "            num_perspectives (int): éœ€è¦èšåˆçš„è§†è§’æ•°é‡ï¼ˆé»˜è®¤3ï¼‰\n",
    "            attn_smoothing (float): æ³¨æ„åŠ›æƒé‡çš„æ ‡ç­¾å¹³æ»‘ç³»æ•°ï¼ˆé»˜è®¤0.1ï¼‰\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.attn_smoothing = attn_smoothing\n",
    "        \n",
    "        # ç¬¬ä¸€çº§èšåˆå™¨ï¼šå¤„ç†æ‰€æœ‰è§†è§’çš„ä¸¤ä¸¤ç»„åˆ\n",
    "        # å…±éœ€è¦n*(n-1)/2ä¸ªèšåˆå™¨ï¼ˆn=è§†è§’æ•°é‡ï¼‰\n",
    "        self.pairwise_aggregators = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size * 2, hidden_size),  # å°†ä¸¤ä¸ªè§†è§’æ‹¼æ¥åçº¿æ€§å˜æ¢\n",
    "                torch.nn.LayerNorm(hidden_size),               # å±‚å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ\n",
    "                torch.nn.GELU()                                # é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»\n",
    "            )\n",
    "            for _ in range(num_perspectives * (num_perspectives - 1) // 2)\n",
    "        ])\n",
    "        \n",
    "        # ç¬¬äºŒçº§èšåˆå™¨ï¼šæ•´åˆæ‰€æœ‰ä¸¤ä¸¤ç»„åˆçš„ç»“æœ\n",
    "        num_pairwise = num_perspectives * (num_perspectives - 1) // 2\n",
    "        self.global_aggregator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_pairwise, hidden_size * 2),  # æ‰©å¤§ç»´åº¦å¢å¼ºè¡¨è¾¾èƒ½åŠ›\n",
    "            torch.nn.LayerNorm(hidden_size * 2),                           # å½’ä¸€åŒ–\n",
    "            torch.nn.GELU(),                                               # éçº¿æ€§æ¿€æ´»\n",
    "            torch.nn.Dropout(0.15),                                        # éšæœºå¤±æ´»é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "            torch.nn.Linear(hidden_size * 2, hidden_size)                  # é™ç»´åˆ°åŸå§‹ç»´åº¦\n",
    "        )\n",
    "        \n",
    "        # æ³¨æ„åŠ›æœºåˆ¶ï¼šè®¡ç®—å„åŸå§‹è§†è§’çš„é‡è¦æ€§æƒé‡\n",
    "        self.attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),  # å‹ç¼©è§†è§’æ‹¼æ¥ä¿¡æ¯\n",
    "            torch.nn.LayerNorm(hidden_size),                               # å½’ä¸€åŒ–\n",
    "            torch.nn.GELU(),                                              # æ¿€æ´»å‡½æ•°\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)                 # è¾“å‡ºå„è§†è§’æƒé‡\n",
    "        )\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # å°†æƒé‡å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è¿‡ç¨‹\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            encoded_responses: å¤šä¸ªè§†è§’çš„ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º[hidden_size]\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            final_output: èšåˆåçš„æœ€ç»ˆç‰¹å¾ [1, hidden_size]\n",
    "            weights: å„è§†è§’çš„æ³¨æ„åŠ›æƒé‡ [1, num_perspectives] \n",
    "        \"\"\"\n",
    "        # å°†å¤šä¸ªè§†è§’ç‰¹å¾å †å ä¸ºå¼ é‡ [num_perspectives, hidden_size]\n",
    "        stacked = torch.stack(encoded_responses)  \n",
    "        \n",
    "        # ç¬¬ä¸€é˜¶æ®µï¼šä¸¤ä¸¤è§†è§’èšåˆ\n",
    "        pairwise_outputs = []\n",
    "        idx = 0  # ç”¨äºé€‰æ‹©å¯¹åº”çš„èšåˆå™¨\n",
    "        \n",
    "        # éå†æ‰€æœ‰ç‹¬ç‰¹çš„è§†è§’ç»„åˆå¯¹\n",
    "        for i in range(self.num_perspectives):\n",
    "            for j in range(i + 1, self.num_perspectives):\n",
    "                # æ‹¼æ¥ä¸¤ä¸ªè§†è§’ç‰¹å¾ [1, hidden_size*2]\n",
    "                pair = torch.cat([stacked[i], stacked[j]], dim=-1).unsqueeze(0)\n",
    "                \n",
    "                # é€šè¿‡å¯¹åº”çš„èšåˆå™¨å¤„ç†\n",
    "                pairwise_outputs.append(self.pairwise_aggregators[idx](pair))\n",
    "                idx += 1\n",
    "        \n",
    "        # ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆ\n",
    "        # æ‹¼æ¥æ‰€æœ‰ä¸¤ä¸¤èšåˆç»“æœ [1, hidden_size*num_pairs]\n",
    "        pairwise_concat = torch.cat(pairwise_outputs, dim=-1)\n",
    "        \n",
    "        # ç”Ÿæˆå…¨å±€èšåˆç‰¹å¾ [1, hidden_size]\n",
    "        global_repr = self.global_aggregator(pairwise_concat)\n",
    "        \n",
    "        # ç¬¬ä¸‰é˜¶æ®µï¼šæ³¨æ„åŠ›æƒé‡è®¡ç®—\n",
    "        # å±•å¹³æ‰€æœ‰åŸå§‹è§†è§’ç‰¹å¾ [1, num_perspectives*hidden_size]\n",
    "        concat = stacked.view(1, -1)\n",
    "        \n",
    "        # è®¡ç®—å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡ [1, num_perspectives]\n",
    "        weights = self.softmax(self.attention(concat))\n",
    "        \n",
    "        # åº”ç”¨æ ‡ç­¾å¹³æ»‘ï¼ˆæ­£åˆ™åŒ–æŠ€æœ¯ï¼‰\n",
    "        if self.attn_smoothing > 0:\n",
    "            uniform_dist = 1 / self.num_perspectives  # å‡åŒ€åˆ†å¸ƒ\n",
    "            weights = weights * (1 - self.attn_smoothing) + self.attn_smoothing * uniform_dist\n",
    "        \n",
    "        # è®¡ç®—åŸå§‹è§†è§’çš„åŠ æƒå’Œ [hidden_size]\n",
    "        weighted_aggregation = torch.sum(\n",
    "            stacked * weights.t().unsqueeze(-1),  # åŠ æƒ\n",
    "            dim=0\n",
    "        )\n",
    "        \n",
    "        # æœ€ç»ˆè¾“å‡ºï¼šå…¨å±€ç‰¹å¾ + åŠ æƒåŸå§‹ç‰¹å¾ [1, hidden_size]\n",
    "        final_output = global_repr + weighted_aggregation.unsqueeze(0)\n",
    "        \n",
    "        return final_output, weights\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"åŠ è½½é¢„è®­ç»ƒçš„BERTç¼–ç å™¨å’Œåˆ†è¯å™¨\"\"\"\n",
    "    model_name = \"D:\\\\xiangyue\\\\model\\\\bert-base-chinese\"\n",
    "    return AutoModel.from_pretrained(model_name), AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡è¡¨ç¤º\n",
    "    ä½¿ç”¨BERTæ¨¡å‹æå–[CLS]æ ‡è®°ä½œä¸ºæ–‡æœ¬çš„æ•´ä½“è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ä»¥æé«˜æ¨ç†æ•ˆç‡\n",
    "        # æå–[CLS]æ ‡è®°çš„åµŒå…¥ä½œä¸ºæ–‡æœ¬è¡¨ç¤º\n",
    "        return model(**inputs).last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm, temperature=0.1):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªè§†è§’ï¼š\n",
    "    1. è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç‰¹å®šè§†è§’çš„å›ç­”\n",
    "    2. å°†å›ç­”ç¼–ç ä¸ºå‘é‡è¡¨ç¤º\n",
    "    è¿”å›åŒ…å«ç´¢å¼•ã€å›ç­”æ–‡æœ¬å’Œç¼–ç å‘é‡çš„å…ƒç»„\n",
    "    \n",
    "    å‚æ•°:\n",
    "        index: è§†è§’ç´¢å¼•\n",
    "        question: é—®é¢˜æ–‡æœ¬\n",
    "        encoder_model: ç¼–ç å™¨æ¨¡å‹\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        llm: å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨å‡½æ•°\n",
    "        temperature: æ§åˆ¶ç”Ÿæˆéšæœºæ€§çš„æ¸©åº¦å€¼\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨ç‰¹å®šè§†è§’å‰ç¼€æç¤ºå¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¼ å…¥temperatureå‚æ•°\n",
    "    response = llm(f\"ã€æ¢ä¸€ç§è§†è§’æ€è€ƒ{index+1}ã€‘{question}\", temperature=temperature)\n",
    "    return index, response, encode_text(response, encoder_model, tokenizer)\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1, llm=None, temperature_settings=None):\n",
    "    \"\"\"\n",
    "    å¤šè§†è§’åˆ†æä¸»å‡½æ•°ï¼š\n",
    "    1. å¹¶è¡Œç”Ÿæˆå¤šä¸ªè§†è§’çš„å›ç­”\n",
    "    2. å¯¹å›ç­”è¿›è¡Œç¼–ç \n",
    "    3. ä½¿ç”¨å¢å¼ºèšåˆå±‚è®¡ç®—å„è§†è§’æƒé‡\n",
    "    4. é€‰æ‹©æƒé‡æœ€é«˜çš„è§†è§’ä½œä¸ºæœ€ç»ˆç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        metaprompt: è¾“å…¥çš„é—®é¢˜/æç¤º\n",
    "        p: è§†è§’æ•°é‡\n",
    "        topk: è¿”å›topkä¸ªæœ€ä½³è§†è§’\n",
    "        llm: å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨å‡½æ•°\n",
    "        temperature_settings: æ¸©åº¦å€¼è®¾ç½®ï¼Œå¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–é•¿åº¦ä¸ºpçš„åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # å¦‚æœæœªæä¾›llmï¼Œåˆ™ä½¿ç”¨å…¨å±€å¯¼å…¥çš„llm_qwen\n",
    "    if llm is None:\n",
    "        llm = llm_qwen\n",
    "    \n",
    "    # å¤„ç†temperatureè®¾ç½®\n",
    "    if temperature_settings is None:\n",
    "        temperatures = [0.1] * p  # é»˜è®¤æ‰€æœ‰è¯·æ±‚ä½¿ç”¨0.1\n",
    "    elif isinstance(temperature_settings, (int, float)):\n",
    "        temperatures = [temperature_settings] * p  # å•ä¸ªå€¼åº”ç”¨åˆ°æ‰€æœ‰è¯·æ±‚\n",
    "    elif isinstance(temperature_settings, (list, tuple)) and len(temperature_settings) == p:\n",
    "        temperatures = temperature_settings  # ä½¿ç”¨æä¾›çš„æ¸©åº¦åˆ—è¡¨\n",
    "    else:\n",
    "        raise ValueError(\"temperature_settings should be a single value or a list of length p\")\n",
    "    \n",
    "    # åˆå§‹åŒ–èšåˆæ¨¡å‹å’Œç¼–ç å™¨\n",
    "    aggregation_model = HierarchicalAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å¤šä¸ªï¼Œæ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„temperature\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, \n",
    "                i, \n",
    "                metaprompt, \n",
    "                encoder_model, \n",
    "                tokenizer, \n",
    "                llm, \n",
    "                temperatures[i]\n",
    "            ) for i in range(p)\n",
    "        ]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    # æŒ‰ç´¢å¼•æ’åºç¡®ä¿é¡ºåºæ­£ç¡®\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    encoded_responses = [enc for _, _, enc in results]  # æå–ç¼–ç å‘é‡\n",
    "    response_texts = [resp for _, resp, _ in results]  # æå–å›ç­”æ–‡æœ¬\n",
    "    for idx, (text, temp) in enumerate(zip(response_texts, temperatures), start=1):\n",
    "        print(f\"ç¬¬{idx}ä¸ª(temperature={temp:.2f})ï¼š{text}\")\n",
    "        \n",
    "    if encoded_responses:\n",
    "        # èšåˆå¤šè§†è§’ä¿¡æ¯å¹¶è·å–æƒé‡\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # æŒ‰æƒé‡æ’åºè§†è§’\n",
    "        sorted_items = sorted([(i, f\"response_{i+1}\", weights[0, i].item()) for i in range(p)], \n",
    "                             key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        # è·å–topkè§†è§’\n",
    "        top_indices = torch.topk(weights[0], topk).indices\n",
    "        top_perspectives = [(f\"response_{idx+1}\", weights[0, idx].item(), response_texts[idx]) \n",
    "                           for idx in top_indices]\n",
    "        \n",
    "        # æ„å»ºæœ€ç»ˆæ ‡é¢˜ï¼ˆå½“å‰å·²æ³¨é‡Šæ‰ï¼‰\n",
    "        final_title = \"\\n\".join(re.sub(r'ã€[^ã€‘]+ã€‘', '', content).strip() for _, _, content in top_perspectives)\n",
    "        \n",
    "        return {\n",
    "            # \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            # \"temperature_settings\": temperatures,  # è¿”å›ä½¿ç”¨çš„æ¸©åº¦è®¾ç½®\n",
    "            # \"all_weights\": [(f\"response_{i+1}\", weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    metaprompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # æ‰‹åŠ¨è®¾ç½®æ¯æ¬¡å¹¶å‘çš„æ¸©åº¦å€¼\n",
    "    manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4,1.8,2]\n",
    "    # manual_temperatures = [0.2, 0.4, 0.6, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.5, 0.8, 1, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 0.8, 1.4, 2]\n",
    "    # manual_temperatures = [0.2, 0.6, 1, 2]\n",
    "    # manual_temperatures = [0.2, 0.8, 2] # å®¢æœæ€»ç»“\n",
    "    # manual_temperatures = [0.2, 2] # topk = 1\n",
    "\n",
    "    result = multi_perspective_analysis(\n",
    "        metaprompt, \n",
    "        p=len(manual_temperatures),  # è‡ªåŠ¨æ ¹æ®æ¸©åº¦åˆ—è¡¨é•¿åº¦ç¡®å®špå€¼\n",
    "        topk=2, \n",
    "        llm=llm_qwen, \n",
    "        temperature_settings=manual_temperatures  # ä¼ å…¥æ‰‹åŠ¨è®¾ç½®çš„æ¸©åº¦åˆ—è¡¨\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))  # ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "class EnhancedAggregationLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    å¢å¼ºå‹èšåˆå±‚ï¼Œç”¨äºå¤šè§†è§’å“åº”çš„è¯­ä¹‰èšåˆ\n",
    "    èåˆParScaleæŠ€æœ¯çš„æ³¨æ„åŠ›å¹³æ»‘æœºåˆ¶ï¼Œæå‡å¤šè§†è§’åˆ†æçš„ç¨³å®šæ€§\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_perspectives=3, parscale_smoothing=0.1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–èšåˆå±‚\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: ç¼–ç å™¨è¾“å‡ºçš„éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤768ï¼ˆBERT-baseé…ç½®ï¼‰\n",
    "            num_perspectives: å¤šè§†è§’åˆ†æçš„è§†è§’æ•°é‡ï¼Œé»˜è®¤3ä¸ªè§†è§’\n",
    "            parscale_smoothing: ParScaleæ³¨æ„åŠ›å¹³æ»‘ç³»æ•°ï¼Œé˜²æ­¢æç«¯æƒé‡ï¼Œé»˜è®¤0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_perspectives = num_perspectives\n",
    "        self.hidden_size = hidden_size\n",
    "        self.parscale_smoothing = parscale_smoothing\n",
    "        \n",
    "        # å®šä¹‰èšåˆç½‘ç»œç»“æ„ï¼šçº¿æ€§å˜æ¢+å±‚å½’ä¸€åŒ–+æ¿€æ´»å‡½æ•°+Dropout+è¾“å‡ºæƒé‡\n",
    "        self.aggregate_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * num_perspectives, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.SiLU(),  # å¹³æ»‘æ¿€æ´»å‡½æ•°ï¼Œæ›¿ä»£ReLU\n",
    "            torch.nn.Dropout(0.1),  # é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "            torch.nn.Linear(hidden_size, num_perspectives)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)  # æƒé‡å½’ä¸€åŒ–\n",
    "        \n",
    "    def forward(self, encoded_responses):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ï¼šè®¡ç®—å¤šè§†è§’å“åº”çš„èšåˆæƒé‡å’Œç»¼åˆè¡¨ç¤º\n",
    "        \n",
    "        Args:\n",
    "            encoded_responses: å„è§†è§’å“åº”çš„ç¼–ç å‘é‡åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º[p, h]\n",
    "        \n",
    "        Returns:\n",
    "            aggregated_output: èšåˆåçš„ç»¼åˆå‘é‡ï¼Œå½¢çŠ¶ä¸º[h]\n",
    "            weights: å„è§†è§’çš„æ³¨æ„åŠ›æƒé‡ï¼Œå½¢çŠ¶ä¸º[1, p]\n",
    "        \"\"\"\n",
    "        stacked_responses = torch.stack(encoded_responses)  # å †å è§†è§’ç¼–ç ä¸º[p, h]\n",
    "        concat_responses = stacked_responses.view(1, -1)  # æ‹¼æ¥ä¸º[1, p*h]ç”¨äºç‰¹å¾æå–\n",
    "        \n",
    "        raw_weights = self.aggregate_layer(concat_responses)  # è®¡ç®—åŸå§‹æƒé‡\n",
    "        weights = self.softmax(raw_weights)  # æƒé‡å½’ä¸€åŒ–\n",
    "        \n",
    "        # åº”ç”¨ParScaleæ³¨æ„åŠ›å¹³æ»‘ï¼šé˜²æ­¢æŸä¸ªè§†è§’æƒé‡è¿‡å¤§\n",
    "        if self.parscale_smoothing > 0:\n",
    "            uniform_weight = 1.0 / self.num_perspectives\n",
    "            weights = weights * (1 - self.parscale_smoothing) + self.parscale_smoothing * uniform_weight\n",
    "        \n",
    "        # åŠ æƒæ±‚å’Œç”Ÿæˆç»¼åˆè¡¨ç¤º\n",
    "        weighted_sum = torch.sum(stacked_responses * weights.t(), dim=0)  # [p, h] * [p, 1] åæ±‚å’Œ\n",
    "        return weighted_sum, weights\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°æœ¬åœ°Qwenæ¨¡å‹æœåŠ¡\n",
    "client = OpenAI(api_key=\"0\", base_url=\"http://192.168.106.26:20000/v1\")\n",
    "\n",
    "def llm_qwen(prompt, model=\"Qwen3\", temperature=0.7, top_p=0.8, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    è°ƒç”¨Qwenæ¨¡å‹APIç”Ÿæˆå“åº”\n",
    "    \n",
    "    Args:\n",
    "        prompt: è¾“å…¥æç¤ºè¯\n",
    "        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤Qwen3\n",
    "        temperature: ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤0.7\n",
    "        top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼Œé»˜è®¤0.8\n",
    "        max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œé»˜è®¤1024\n",
    "    \n",
    "    Returns:\n",
    "        æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å“åº”\n",
    "    \"\"\"\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def load_encoder_model():\n",
    "    \"\"\"\n",
    "    åŠ è½½æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ï¼ˆBERT-baseï¼‰ç”¨äºè¯­ä¹‰å‘é‡åŒ–\n",
    "    \n",
    "    Returns:\n",
    "        model: BERTæ¨¡å‹å®ä¾‹\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨\n",
    "    \"\"\"\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    model_name = \"/app/sda1/xiangyue/model/bert-base-chinese\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def encode_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡è¡¨ç¤ºï¼ˆä½¿ç”¨BERTçš„[CLS]æ ‡è®°ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        text: è¾“å…¥æ–‡æœ¬\n",
    "        model: BERTæ¨¡å‹å®ä¾‹\n",
    "        tokenizer: åˆ†è¯å™¨å®ä¾‹\n",
    "    \n",
    "    Returns:\n",
    "        æ–‡æœ¬çš„è¯­ä¹‰ç¼–ç å‘é‡ï¼Œå½¢çŠ¶ä¸º[hidden_size]\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():  # æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—\n",
    "        outputs = model(**inputs)\n",
    "    # ä½¿ç”¨BERTçš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰ä½œä¸ºæ•´ä½“è¯­ä¹‰è¡¨ç¤º\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "\n",
    "def process_perspective(index, question, encoder_model, tokenizer, llm_qwen):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªè§†è§’ï¼šç”Ÿæˆå“åº”å¹¶è¿›è¡Œè¯­ä¹‰ç¼–ç \n",
    "    \n",
    "    Args:\n",
    "        index: è§†è§’ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰\n",
    "        question: åˆ†æé—®é¢˜\n",
    "        encoder_model: ç¼–ç å™¨æ¨¡å‹\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        llm_qwen: LLMè°ƒç”¨å‡½æ•°\n",
    "    \n",
    "    Returns:\n",
    "        index: è§†è§’ç´¢å¼•\n",
    "        response: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬\n",
    "        encoded_response: å“åº”çš„è¯­ä¹‰ç¼–ç \n",
    "    \"\"\"\n",
    "    # ä¸ºæ¯ä¸ªè§†è§’æ·»åŠ æ ‡è¯†ï¼Œå¼•å¯¼æ¨¡å‹ä»ä¸åŒè§’åº¦æ€è€ƒ\n",
    "    perspective_prompt = f\"ã€è§†è§’{index+1}ã€‘{question}\"\n",
    "    response = llm_qwen(perspective_prompt)  # è°ƒç”¨Qwenæ¨¡å‹ç”Ÿæˆå“åº”\n",
    "    encoded_response = encode_text(response, encoder_model, tokenizer)  # ç¼–ç å“åº”æ–‡æœ¬\n",
    "    return index, response, encoded_response\n",
    "\n",
    "\n",
    "def multi_perspective_analysis(metaprompt, p=3, topk=1):\n",
    "    \"\"\"\n",
    "    å¤šè§†è§’åˆ†æä¸»å‡½æ•°ï¼šç”Ÿæˆå¤šä¸ªè§†è§’å“åº”å¹¶èšåˆåˆ†æç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        metaprompt: å…ƒæç¤ºè¯ï¼ˆåˆ†æé—®é¢˜ï¼‰\n",
    "        p: ç”Ÿæˆçš„è§†è§’æ•°é‡ï¼Œé»˜è®¤3ä¸ª\n",
    "        topk: ä¿ç•™çš„å…³é”®è§†è§’æ•°é‡ï¼Œé»˜è®¤1ä¸ª\n",
    "    \n",
    "    Returns:\n",
    "        åŒ…å«èšåˆç»“æœã€å…³é”®è§†è§’å’Œæ‰€æœ‰è§†è§’æƒé‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–èšåˆæ¨¡å‹ä¸ç¼–ç å™¨\n",
    "    aggregation_model = EnhancedAggregationLayer(hidden_size=768, num_perspectives=p)\n",
    "    encoder_model, tokenizer = load_encoder_model()\n",
    "    \n",
    "    # å¹¶è¡Œå¤„ç†å¤šä¸ªè§†è§’ï¼ˆä½¿ç”¨çº¿ç¨‹æ± æé«˜æ•ˆç‡ï¼‰\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=p) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                process_perspective, i, metaprompt, encoder_model, tokenizer, llm_qwen\n",
    "            )\n",
    "            for i in range(p)\n",
    "        ]\n",
    "        # æ”¶é›†æ‰€æœ‰è§†è§’çš„å¤„ç†ç»“æœ\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    # æŒ‰è§†è§’ç´¢å¼•æ’åºç»“æœ\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    # æå–å“åº”æ–‡æœ¬å’Œç¼–ç å‘é‡\n",
    "    responses = [(f\"response_{i+1}\", resp) for i, (_, resp, _) in enumerate(results)]\n",
    "    encoded_responses = [enc for _, _, enc in results]\n",
    "    response_texts = [resp for _, resp, _ in results]\n",
    "    \n",
    "    # æ‰“å°åŸå§‹å“åº”ï¼ˆè°ƒè¯•å’Œå¯è§†åŒ–ï¼‰\n",
    "    print(\"=== åŸå§‹å“åº” ===\")\n",
    "    for i, (key, response) in enumerate(responses):\n",
    "        print(f\"{key}: {response}\")\n",
    "    \n",
    "    # æ‰§è¡Œèšåˆé€»è¾‘ï¼ˆä»…å½“æœ‰æœ‰æ•ˆå“åº”æ—¶ï¼‰\n",
    "    if encoded_responses:\n",
    "        aggregated_output, weights = aggregation_model(encoded_responses)\n",
    "        \n",
    "        # æ‰“å°å„è§†è§’çš„èšåˆæƒé‡\n",
    "        print(\"\\n=== èšåˆæƒé‡ ===\")\n",
    "        for i, (key, _) in enumerate(responses):\n",
    "            print(f\"{key}: {weights[0, i].item():.4f}\")\n",
    "        \n",
    "        # è·å–æƒé‡æœ€é«˜çš„topkä¸ªè§†è§’\n",
    "        top_weights, top_indices = torch.topk(weights[0], topk)\n",
    "        print(f\"\\n=== Top {topk} è§†è§’ ===\")\n",
    "        top_perspectives = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            key = responses[idx][0]\n",
    "            weight = top_weights[i].item()\n",
    "            content = response_texts[idx]\n",
    "            print(f\"{key}: æƒé‡={weight:.4f}\\nå†…å®¹: {content}\")\n",
    "            top_perspectives.append((key, weight, content))\n",
    "        \n",
    "        # ç”Ÿæˆæœ€ç»ˆèšåˆç»“æœï¼ˆåˆå¹¶å…³é”®è§†è§’å†…å®¹ï¼‰\n",
    "        final_title = \"\\n\".join(re.sub(r'ã€[^ã€‘]+ã€‘', '', content).strip() for _, _, content in top_perspectives)\n",
    "        print(\"\\n=== æœ€ç»ˆèšåˆç»“æœ ===\")\n",
    "        print(final_title)\n",
    "        \n",
    "        return {\n",
    "            \"final_title\": final_title,\n",
    "            \"top_perspectives\": top_perspectives,\n",
    "            \"all_weights\": [(responses[i][0], weights[0, i].item()) for i in range(p)]\n",
    "        }\n",
    "    else:\n",
    "        print(\"æ²¡æœ‰æœ‰æ•ˆçš„å›ç­”å¯ä¾›èšåˆ\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ç¤ºä¾‹å…ƒæç¤ºè¯ï¼ˆç©ºæç¤ºç”¨äºæµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼‰\n",
    "    metaprompt = \"\"\"\n",
    "/no_think\n",
    "\"\"\"\n",
    "    # æ‰§è¡Œå¤šè§†è§’åˆ†æï¼ˆç”Ÿæˆ3ä¸ªè§†è§’ï¼Œä¿ç•™1ä¸ªå…³é”®è§†è§’ï¼‰\n",
    "    result = multi_perspective_analysis(metaprompt, p=3, topk=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
